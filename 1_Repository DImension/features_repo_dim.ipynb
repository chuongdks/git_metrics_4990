{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05532dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b02dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\", columns=['repo_id', 'merged_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36c04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2807 repos down to 86 using language='Java'.\n"
     ]
    }
   ],
   "source": [
    "# Filter for Repo that has Java language\n",
    "initial_count = len(repo_df)\n",
    "repo_df = repo_df[repo_df['language'] == 'Java']\n",
    "print(f\"Filtered {initial_count} repos down to {len(repo_df)} using language='Java'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63114452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for 86 unique repositories ready for merge.\n"
     ]
    }
   ],
   "source": [
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "        repo_df[['id', 'full_name']], \n",
    "        left_on='repo_id', \n",
    "        right_on='id', \n",
    "        how='inner' \n",
    "    ).drop(columns=['id', 'repo_id'])\n",
    "\n",
    "# Filter PRs that were rejected (not merged)\n",
    "merged_pr_df['is_accepted'] = merged_pr_df['merged_at'].notnull()\n",
    "\n",
    "# Group and Aggregate: Group by the new 'full_name' column\n",
    "repo_metrics = merged_pr_df.groupby('full_name').agg(\n",
    "    total_prs=('is_accepted', 'size'),        \n",
    "    accepted_prs=('is_accepted', 'sum')       \n",
    ").reset_index()\n",
    "\n",
    "# Calculate Rejected: Subtract accepted from total\n",
    "repo_metrics['rejected_prs'] = repo_metrics['total_prs'] - repo_metrics['accepted_prs']\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "repo_metrics = repo_metrics[['full_name', 'accepted_prs', 'rejected_prs']]\n",
    "\n",
    "# print to csv for checking\n",
    "print(f\"Aggregated metrics for {len(repo_metrics)} unique repositories ready for merge.\")\n",
    "repo_metrics.to_csv(\"accepted_rejected_repo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0c5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepted PR if accepted_pr >= rejected_pr,\n",
    "repo_metrics['pr_status'] = np.where(\n",
    "    repo_metrics['accepted_prs'] >= repo_metrics['rejected_prs'], \n",
    "    'Accepted', \n",
    "    'Rejected'\n",
    ")\n",
    "\n",
    "# Filter the DataFrame into two parts\n",
    "accepted_repos_df = repo_metrics[repo_metrics['pr_status'] == 'Accepted']\n",
    "rejected_repos_df = repo_metrics[repo_metrics['pr_status'] == 'Rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfa6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 entries of the ACCEPTED REPOSITORIES list:\n",
      "[('Camelcade', 'Perl5-IDEA'), ('DataDog', 'dd-trace-java'), ('EduMIPS64', 'edumips64'), ('JetBrains', 'psiviewer'), ('OWASP', 'wrongsecrets')]\n",
      "\n",
      "First 5 entries of the REJECTED REPOSITORIES list:\n",
      "[('1c-syntax', 'bsl-language-server'), ('2006-Scape', '2006Scape'), ('AutoMQ', 'automq'), ('Azure-Samples', 'azure-search-openai-demo-java'), ('Azure', 'azure-sdk-for-java')]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(repo_metrics_df, status_filter):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    # 1. Filter the DataFrame by status\n",
    "    repos_df = repo_metrics_df[repo_metrics_df['pr_status'] == status_filter]\n",
    "    \n",
    "    # 2. Split the name into (owner, repo) tuples\n",
    "    # Use .str.split().apply(tuple) to get a list of tuples directly (more Pythonic)\n",
    "    repositories = repos_df['full_name'].str.split('/', n=1, expand=True).apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    print(f\"\\nFirst 5 entries of the {status_filter.upper()} REPOSITORIES list:\")\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_REPOSITORIES = process_repositories(repo_metrics, 'Accepted')\n",
    "REJECTED_REPOSITORIES = process_repositories(repo_metrics, 'Rejected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451c76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n",
      "Error fetching contributors for SimonHalvdansson/Harmonic-HN: 429 Client Error: too many requests for url: https://api.github.com/repos/SimonHalvdansson/Harmonic-HN/contributors?per_page=1&anon=true\n",
      "Error fetching data for Stirling-Tools/Stirling-PDF: 429 Client Error: too many requests for url: https://api.github.com/repos/Stirling-Tools/Stirling-PDF\n",
      "Error fetching data for USS-Shenzhou/MadParticle: 429 Client Error: too many requests for url: https://api.github.com/repos/USS-Shenzhou/MadParticle\n",
      "Error fetching data for VerisimilitudeX/DNAnalyzer: 429 Client Error: too many requests for url: https://api.github.com/repos/VerisimilitudeX/DNAnalyzer\n",
      "Error fetching data for W1LDN16H7/JPL: 429 Client Error: too many requests for url: https://api.github.com/repos/W1LDN16H7/JPL\n",
      "Error fetching data for apache/pinot: 429 Client Error: too many requests for url: https://api.github.com/repos/apache/pinot\n",
      "Error fetching data for apache/pulsar: 429 Client Error: too many requests for url: https://api.github.com/repos/apache/pulsar\n",
      "Error fetching data for cwuom/ono: 429 Client Error: too many requests for url: https://api.github.com/repos/cwuom/ono\n",
      "Error fetching data for datahub-project/datahub: 429 Client Error: too many requests for url: https://api.github.com/repos/datahub-project/datahub\n",
      "Error fetching data for freeplane/freeplane: 429 Client Error: too many requests for url: https://api.github.com/repos/freeplane/freeplane\n",
      "Error fetching data for halilozel1903/AndroidTVMovieParadise: 429 Client Error: too many requests for url: https://api.github.com/repos/halilozel1903/AndroidTVMovieParadise\n",
      "Error fetching data for jdereg/java-util: 429 Client Error: too many requests for url: https://api.github.com/repos/jdereg/java-util\n",
      "Error fetching data for jdereg/json-io: 429 Client Error: too many requests for url: https://api.github.com/repos/jdereg/json-io\n",
      "Error fetching data for kongzue/DialogX: 429 Client Error: too many requests for url: https://api.github.com/repos/kongzue/DialogX\n",
      "Error fetching data for lunasaw/gb28181-proxy: 429 Client Error: too many requests for url: https://api.github.com/repos/lunasaw/gb28181-proxy\n",
      "Error fetching data for objectionary/eo: 429 Client Error: too many requests for url: https://api.github.com/repos/objectionary/eo\n",
      "Error fetching data for operaton/operaton: 429 Client Error: too many requests for url: https://api.github.com/repos/operaton/operaton\n",
      "Error fetching data for plantuml/plantuml: 429 Client Error: too many requests for url: https://api.github.com/repos/plantuml/plantuml\n",
      "Error fetching data for sakaiproject/sakai: 429 Client Error: too many requests for url: https://api.github.com/repos/sakaiproject/sakai\n",
      "Error fetching data for traccar/traccar: 429 Client Error: too many requests for url: https://api.github.com/repos/traccar/traccar\n",
      "Error fetching data for valkey-io/valkey-glide: 429 Client Error: too many requests for url: https://api.github.com/repos/valkey-io/valkey-glide\n",
      "Error fetching data for wgzhao/Addax: 429 Client Error: too many requests for url: https://api.github.com/repos/wgzhao/Addax\n",
      "Error fetching data for wgzhao/dbeaver-agent: 429 Client Error: too many requests for url: https://api.github.com/repos/wgzhao/dbeaver-agent\n",
      "Error fetching data for xpinjection/test-driven-spring-boot: 429 Client Error: too many requests for url: https://api.github.com/repos/xpinjection/test-driven-spring-boot\n",
      "Error fetching data for 1c-syntax/bsl-language-server: 429 Client Error: too many requests for url: https://api.github.com/repos/1c-syntax/bsl-language-server\n",
      "Error fetching data for 2006-Scape/2006Scape: 429 Client Error: too many requests for url: https://api.github.com/repos/2006-Scape/2006Scape\n",
      "Error fetching data for AutoMQ/automq: 429 Client Error: too many requests for url: https://api.github.com/repos/AutoMQ/automq\n",
      "Error fetching data for Azure-Samples/azure-search-openai-demo-java: 429 Client Error: too many requests for url: https://api.github.com/repos/Azure-Samples/azure-search-openai-demo-java\n",
      "Error fetching data for Azure/azure-sdk-for-java: 429 Client Error: too many requests for url: https://api.github.com/repos/Azure/azure-sdk-for-java\n",
      "Error fetching data for CarGuo/GSYVideoPlayer: 429 Client Error: too many requests for url: https://api.github.com/repos/CarGuo/GSYVideoPlayer\n",
      "Error fetching data for GrimAnticheat/Grim: 429 Client Error: too many requests for url: https://api.github.com/repos/GrimAnticheat/Grim\n",
      "Error fetching data for Igalia/wolvic: 429 Client Error: too many requests for url: https://api.github.com/repos/Igalia/wolvic\n",
      "Error fetching data for Kaljurand/K6nele: 429 Client Error: too many requests for url: https://api.github.com/repos/Kaljurand/K6nele\n",
      "Error fetching data for MeteorDevelopment/meteor-client: 429 Client Error: too many requests for url: https://api.github.com/repos/MeteorDevelopment/meteor-client\n",
      "Error fetching data for RyanSusana/elepy: 429 Client Error: too many requests for url: https://api.github.com/repos/RyanSusana/elepy\n",
      "Error fetching data for StarRocks/starrocks: 429 Client Error: too many requests for url: https://api.github.com/repos/StarRocks/starrocks\n",
      "Error fetching data for YunaiV/ruoyi-vue-pro: 429 Client Error: too many requests for url: https://api.github.com/repos/YunaiV/ruoyi-vue-pro\n",
      "Error fetching data for YunaiV/yudao-cloud: 429 Client Error: too many requests for url: https://api.github.com/repos/YunaiV/yudao-cloud\n",
      "Error fetching data for akto-api-security/akto: 429 Client Error: too many requests for url: https://api.github.com/repos/akto-api-security/akto\n",
      "Error fetching data for apache/ozone: 429 Client Error: too many requests for url: https://api.github.com/repos/apache/ozone\n",
      "Error fetching data for apolloconfig/apollo: 429 Client Error: too many requests for url: https://api.github.com/repos/apolloconfig/apollo\n",
      "Error fetching data for binarywang/WxJava: 429 Client Error: too many requests for url: https://api.github.com/repos/binarywang/WxJava\n",
      "Error fetching data for camunda/camunda: 429 Client Error: too many requests for url: https://api.github.com/repos/camunda/camunda\n",
      "Error fetching data for chsami/Microbot: 429 Client Error: too many requests for url: https://api.github.com/repos/chsami/Microbot\n",
      "Error fetching data for dji-sdk/Mobile-SDK-Android-V5: 429 Client Error: too many requests for url: https://api.github.com/repos/dji-sdk/Mobile-SDK-Android-V5\n",
      "Error fetching data for domaframework/doma: 429 Client Error: too many requests for url: https://api.github.com/repos/domaframework/doma\n",
      "Error fetching data for dotCMS/core: 429 Client Error: too many requests for url: https://api.github.com/repos/dotCMS/core\n",
      "Error fetching data for feiyuchuixue/sz-boot-parent: 429 Client Error: too many requests for url: https://api.github.com/repos/feiyuchuixue/sz-boot-parent\n",
      "Error fetching data for geoserver/geoserver: 429 Client Error: too many requests for url: https://api.github.com/repos/geoserver/geoserver\n",
      "Error fetching data for halo-dev/halo: 429 Client Error: too many requests for url: https://api.github.com/repos/halo-dev/halo\n",
      "Error fetching data for hyperledger/besu: 429 Client Error: too many requests for url: https://api.github.com/repos/hyperledger/besu\n",
      "Error fetching data for hyperxpro/Brotli4j: 429 Client Error: too many requests for url: https://api.github.com/repos/hyperxpro/Brotli4j\n",
      "Error fetching data for ikarus23/MifareClassicTool: 429 Client Error: too many requests for url: https://api.github.com/repos/ikarus23/MifareClassicTool\n",
      "Error fetching data for jbangdev/jbang: 429 Client Error: too many requests for url: https://api.github.com/repos/jbangdev/jbang\n",
      "Error fetching data for jenkinsci/jira-plugin: 429 Client Error: too many requests for url: https://api.github.com/repos/jenkinsci/jira-plugin\n",
      "Error fetching data for liyupi/yu-ai-agent: 429 Client Error: too many requests for url: https://api.github.com/repos/liyupi/yu-ai-agent\n",
      "Error fetching data for microsoft/ApplicationInsights-Java: 429 Client Error: too many requests for url: https://api.github.com/repos/microsoft/ApplicationInsights-Java\n",
      "Error fetching data for microsoft/HydraLab: 429 Client Error: too many requests for url: https://api.github.com/repos/microsoft/HydraLab\n",
      "Error fetching data for microsoft/semantic-kernel-java: 429 Client Error: too many requests for url: https://api.github.com/repos/microsoft/semantic-kernel-java\n",
      "Error fetching data for microsoft/typespec: 429 Client Error: too many requests for url: https://api.github.com/repos/microsoft/typespec\n",
      "Error fetching data for mixpanel/mixpanel-android: 429 Client Error: too many requests for url: https://api.github.com/repos/mixpanel/mixpanel-android\n",
      "Error fetching data for nuxeo/nuxeo: 429 Client Error: too many requests for url: https://api.github.com/repos/nuxeo/nuxeo\n",
      "Error fetching data for qaiu/netdisk-fast-download: 429 Client Error: too many requests for url: https://api.github.com/repos/qaiu/netdisk-fast-download\n",
      "Error fetching data for rieckpil/testing-spring-boot-applications-masterclass: 429 Client Error: too many requests for url: https://api.github.com/repos/rieckpil/testing-spring-boot-applications-masterclass\n",
      "Error fetching data for spring-projects/spring-kafka: 429 Client Error: too many requests for url: https://api.github.com/repos/spring-projects/spring-kafka\n",
      "Error fetching data for stingle/stingle-photos-android: 429 Client Error: too many requests for url: https://api.github.com/repos/stingle/stingle-photos-android\n",
      "Error fetching data for tonihele/OpenKeeper: 429 Client Error: too many requests for url: https://api.github.com/repos/tonihele/OpenKeeper\n",
      "Error fetching data for trinodb/trino: 429 Client Error: too many requests for url: https://api.github.com/repos/trinodb/trino\n",
      "Error fetching data for vaadin/flow: 429 Client Error: too many requests for url: https://api.github.com/repos/vaadin/flow\n",
      "Error fetching data for why168/LoopViewPagerLayout: 429 Client Error: too many requests for url: https://api.github.com/repos/why168/LoopViewPagerLayout\n",
      "Error fetching data for wso2/product-is: 429 Client Error: too many requests for url: https://api.github.com/repos/wso2/product-is\n",
      "Error fetching data for yegor256/cactoos: 429 Client Error: too many requests for url: https://api.github.com/repos/yegor256/cactoos\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Total Contributors\n",
    "# ============================================================\n",
    "def get_total_contributors(owner, repo, headers):\n",
    "    \"\"\"\n",
    "    Retrieves the total number of contributors for a GitHub repository \n",
    "    using the Link header pagination trick.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "    params = {\"per_page\": 1, \"anon\": \"true\"}  # include anonymous contributors\n",
    "\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        link_header = response.headers.get('Link')\n",
    "\n",
    "        if link_header:\n",
    "            match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        # fallback for small repos\n",
    "        return 1\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching contributors for {owner}/{repo}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper: Commit Count\n",
    "# ============================================================\n",
    "def get_commit_count(owner, repo, headers):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    params = {\"per_page\": 1}\n",
    "\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        link_header = response.headers.get('Link')\n",
    "\n",
    "        if link_header:\n",
    "            match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "\n",
    "        # fallback for repos with few commits\n",
    "        response = requests.get(url, headers=headers, params={\"per_page\": 100}, timeout=10)\n",
    "        return len(response.json())\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching commits for {owner}/{repo}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper: Issues and Pull Requests\n",
    "# ============================================================\n",
    "def get_issue_and_pull_metrics(owner, repo, headers):\n",
    "    base_search_url = \"https://api.github.com/search/issues\"\n",
    "\n",
    "    queries = {\n",
    "        'open_issues': f\"repo:{owner}/{repo} is:issue is:open\",\n",
    "        'closed_issues': f\"repo:{owner}/{repo} is:issue is:closed\",\n",
    "        'open_prs': f\"repo:{owner}/{repo} is:pr is:open\",\n",
    "        'closed_prs': f\"repo:{owner}/{repo} is:pr is:closed\",\n",
    "    }\n",
    "\n",
    "    metrics = {'open_issues': 0, 'closed_issues': 0, 'total_pull_requests': 0}\n",
    "\n",
    "    for key, query in queries.items():\n",
    "        try:\n",
    "            response = requests.get(base_search_url, headers=headers, params={'q': query, 'per_page': 1}, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            total = response.json().get('total_count', 0)\n",
    "\n",
    "            if key.startswith('open_issue'):\n",
    "                metrics['open_issues'] = total\n",
    "            elif key.startswith('closed_issue'):\n",
    "                metrics['closed_issues'] = total\n",
    "            else:\n",
    "                metrics['total_pull_requests'] += total\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ============================================================\n",
    "# Main Function: Repository Metrics\n",
    "# ============================================================\n",
    "def get_repo_metrics(owner, repo, github_token=None):\n",
    "    \"\"\"\n",
    "    Retrieves metrics for a GitHub repository:\n",
    "    Watchers, Duration, Commits, Issues, Pull Requests, Contributors\n",
    "    \"\"\"\n",
    "    repo_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "    headers = {}\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(repo_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        repo_data = response.json()\n",
    "\n",
    "        # Basic metrics\n",
    "        num_watchers = repo_data.get('stargazers_count', 0)\n",
    "        num_commits = get_commit_count(owner, repo, headers)\n",
    "        issue_pr_metrics = get_issue_and_pull_metrics(owner, repo, headers)\n",
    "        num_closed_issues = issue_pr_metrics['closed_issues']\n",
    "        num_open_issues = issue_pr_metrics['open_issues']\n",
    "        num_pull_reqs = issue_pr_metrics['total_pull_requests']\n",
    "        num_contributors = get_total_contributors(owner, repo, headers)\n",
    "\n",
    "        # Duration (days since creation)\n",
    "        created_at_str = repo_data.get('created_at')\n",
    "        num_days = 0\n",
    "        if created_at_str:\n",
    "            created_at_aware = parser.parse(created_at_str)\n",
    "            now_utc = datetime.now(timezone.utc)\n",
    "            num_days = (now_utc - created_at_aware).days\n",
    "\n",
    "        return {\n",
    "            \"Repo\": f\"{owner}/{repo}\",\n",
    "            \"NumWatchers\": num_watchers,\n",
    "            \"NumDays\": num_days,\n",
    "            \"NumCommits\": num_commits,\n",
    "            \"NumOpenIssues\": num_open_issues,\n",
    "            \"NumClosedIssues\": num_closed_issues,\n",
    "            \"NumPullReqs\": num_pull_reqs,\n",
    "            \"NumContributors\": num_contributors,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {owner}/{repo}: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ============================================================\n",
    "# Main Helper: Fetch the main metric functions \n",
    "# ============================================================\n",
    "def fetch_metrics(repo_list, token):\n",
    "    results = []\n",
    "    # limit the number of repositories processed here for testing REPOSITORIES[:10]:\n",
    "    for owner, repo in repo_list: # Apply the test limit here\n",
    "        metrics = get_repo_metrics(owner, repo, token)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    \n",
    "    # Create the Metric DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "repo_metrics_df_accept = fetch_metrics(ACCEPTED_REPOSITORIES, GITHUB_TOKEN)\n",
    "repo_metrics_df_reject = fetch_metrics(REJECTED_REPOSITORIES, GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d20c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n",
      "\n",
      "Accepted Repository Metrics DataFrame Created:\n",
      "Total rows in Accepted DataFrame: 15\n",
      "\n",
      "--- Processing Rejected Repositories ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Repo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10900\\402521983.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mTotal rows in Accepted DataFrame: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_df_accept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# --- Processing Rejected Repositories ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n--- Processing Rejected Repositories ---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m final_df_reject = finalize_dataframe(\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mrepo_metrics_df_reject\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mrepo_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;34m\"repo_metrics_rejected.csv\"\u001b[0m \u001b[1;31m# Save to a separate file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10900\\402521983.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(metrics_df, repo_df, output_filename)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mApplies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmerging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenaming\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mreordering\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 1. Merge with the original repo_df to get stars and forks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     final_df = pd.merge(\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmetrics_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mrepo_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stars'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'forks'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Repo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Repo'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, repo_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Merge with the original repo_df to get stars and forks\n",
    "    final_df = pd.merge(\n",
    "        metrics_df, \n",
    "        repo_df[['full_name', 'stars', 'forks']],\n",
    "        left_on='Repo', \n",
    "        right_on='full_name', \n",
    "        how='left' \n",
    "    )\n",
    "\n",
    "    # 2. Clean up and reorder columns\n",
    "    final_df = final_df.drop(columns=['full_name'])\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'NumWatchers': 'Watchers',\n",
    "        'NumDays': 'Duration_Days',\n",
    "        'NumCommits': 'Commits', \n",
    "        'NumOpenIssues': 'Open_Issues', \n",
    "        'NumClosedIssues': 'Closed_Issues', \n",
    "        'NumPullReqs': 'Pull_Requests', \n",
    "        'NumContributors': 'Contributors',\n",
    "        'stars': 'Stars',\n",
    "        'forks': 'Forks' \n",
    "    }\n",
    "    final_df = final_df.rename(columns=rename_map)\n",
    "\n",
    "    # 3. Define the final column order\n",
    "    column_order = [\n",
    "        'Repo', 'Contributors', 'Stars', 'Forks', 'Watchers', 'Duration_Days', \n",
    "        'Commits', 'Open_Issues', 'Closed_Issues', 'Pull_Requests', \n",
    "    ]\n",
    "    \n",
    "    # Apply the final column order\n",
    "    final_df = final_df[column_order]\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    repo_metrics_df_accept, \n",
    "    repo_df, \n",
    "    \"repo_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    repo_metrics_df_reject, \n",
    "    repo_df, \n",
    "    \"repo_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
