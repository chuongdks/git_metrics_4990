{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da340c98",
   "metadata": {},
   "source": [
    "# Import the Hao-Li AIDev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2df0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df6a28",
   "metadata": {},
   "source": [
    "# 1. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89954997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the repository data for 'Java' language\n",
    "java_repo_df = repo_df[repo_df['language'] == 'Java'].copy()\n",
    "java_repo_select_df = java_repo_df[['id', 'full_name']]\n",
    "\n",
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "    java_repo_select_df,\n",
    "    left_on='repo_id',\n",
    "    right_on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# clean up extra attribute\n",
    "merged_pr_df = merged_pr_df.drop(columns=['id_y'])\n",
    "merged_pr_df = merged_pr_df.rename(columns={'id_x':'id'})\n",
    "\n",
    "# Filter PRs that were rejected (not merged) and create a new attribute\n",
    "accepted_prs = merged_pr_df[merged_pr_df['merged_at'].notnull()]\n",
    "rejected_prs = merged_pr_df[merged_pr_df['merged_at'].isnull()]\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "accepted_prs = accepted_prs[['full_name', 'number']]\n",
    "rejected_prs = rejected_prs[['full_name', 'number']]\n",
    "\n",
    "# print to csv for checking\n",
    "accepted_prs.to_csv(\"accepted_PR.csv\", index=False)\n",
    "rejected_prs.to_csv(\"rejected_PR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13051",
   "metadata": {},
   "source": [
    "## 1.1. Split the full_name of repo into owner and repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14daec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dotCMS', 'core', 32609), ('apache', 'pulsar', 24542), ('dotCMS', 'core', 32771), ('dotCMS', 'core', 32561), ('microsoft', 'ApplicationInsights-Java', 4293)]\n",
      "[('dotCMS', 'core', 32656), ('dotCMS', 'core', 32657), ('dotCMS', 'core', 32658), ('dotCMS', 'core', 32659), ('dotCMS', 'core', 32660)]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(pr_df):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Split the 'full_name' column into 'owner' and 'repo' columns\n",
    "    split_df = pr_df['full_name'].str.split('/', n=1, expand=True)\n",
    "    split_df.columns = ['owner', 'repo']\n",
    "    \n",
    "    # 2. Combine the split columns and the 'number' column into a list of tuples\n",
    "    # We use axis=1 to apply the tuple creation row-wise across the three columns\n",
    "    repositories = pd.concat([split_df, pr_df['number']], axis=1).apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    # Print the first 5 results for verification\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_PULL_REQUEST = process_repositories(accepted_prs)\n",
    "REJECTED_PULL_REQUEST = process_repositories(rejected_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5482fa",
   "metadata": {},
   "source": [
    "# 2. Helper code block to limit the API rate request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "def safe_request(method, url, headers=None, params=None, timeout=10, sleep_between=0.4):\n",
    "    \"\"\"\n",
    "    A rate-limit-safe GitHub request wrapper that handles:\n",
    "    - Primary rate limits (5000/hour)\n",
    "    - Secondary abuse limits (burst protection)\n",
    "    - GET and HEAD requests\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = requests.request(method, url, headers=headers, params=params, timeout=timeout)\n",
    "\n",
    "        # ============================================================\n",
    "        # Rate Limit handling\n",
    "        # ============================================================\n",
    "        # Primary rate limit\n",
    "        remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 1))\n",
    "        reset_ts = int(response.headers.get(\"X-RateLimit-Reset\", time.time()))\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait = max(reset_ts - int(time.time()), 10)\n",
    "            print(f\"[Primary Limit] Waiting {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # Secondary rate limit (abuse detection)\n",
    "        if response.status_code == 403:\n",
    "            print(\"[Secondary Limit] Hit GitHub abuse limit. Backing off 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        \n",
    "        # ============================================================\n",
    "        # API cache\n",
    "        # ============================================================\n",
    "        # Check if the response came from the cache\n",
    "        if hasattr(response, 'from_cache') and response.from_cache:\n",
    "            print(f\"[CACHE] Hit for {url}\")\n",
    "            # Skip the time.sleep(sleep_between) if it came from the cache\n",
    "            return response\n",
    "\n",
    "        # ============================================================\n",
    "        # Network handling\n",
    "        # ============================================================\n",
    "        # Success or other errors handled normally\n",
    "        if not response.ok:\n",
    "            response.raise_for_status()\n",
    "\n",
    "        # Small delay prevents triggering secondary limit\n",
    "        time.sleep(sleep_between)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf896a",
   "metadata": {},
   "source": [
    "# 3. Git API to extract metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n",
      "[Primary Limit] Waiting 3552 seconds...\n",
      "[Primary Limit] Waiting 3553 seconds...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Total reviews (not inline) for a PR\n",
    "# ============================================================\n",
    "def get_review_count(owner: str, repo: str, pr_number: int, headers: Dict) -> int:\n",
    "    \"\"\"Retrieves the total count of formal reviews submitted for a Pull Request using the dedicated /reviews endpoint.\"\"\"\n",
    "    reviews_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\"\n",
    "    \n",
    "    try:\n",
    "        # We use a HEAD request with per_page=1 and pagination trick to get the total count\n",
    "        response = safe_request(\"HEAD\", reviews_url, headers=headers, params={\"per_page\": 1})\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check the 'Link' header for the last page\n",
    "        link_header = response.headers.get('Link')\n",
    "        if link_header:\n",
    "            last_page_match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if last_page_match:\n",
    "                return int(last_page_match.group(1))\n",
    "        \n",
    "    except requests.exceptions.RequestException:\n",
    "        return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper: Path files of a repo\n",
    "# ============================================================\n",
    "def get_file_path_metrics(owner: str, repo: str, pr_number: int, headers: Dict) -> Tuple[int, float, int]:\n",
    "    \"\"\"\n",
    "    Retrieves the count of changed files and calculates file path length statistics.\n",
    "    Returns: (total_files, avg_path_length, max_path_length)\n",
    "    \"\"\"\n",
    "    files_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
    "    all_file_paths = []\n",
    "    page = 1\n",
    "    \n",
    "    # \n",
    "    while True:\n",
    "        try:\n",
    "            response = safe_request(\"GET\", files_url, headers=headers, params={\"per_page\": 100, \"page\": page})\n",
    "            response.raise_for_status()\n",
    "            files_data = response.json()\n",
    "            \n",
    "            if not files_data:\n",
    "                break\n",
    "                \n",
    "            # Extract the 'filename' (which includes the full path)\n",
    "            for file in files_data:\n",
    "                # Store the length of the full file path string\n",
    "                all_file_paths.append(len(file.get('filename', ''))) \n",
    "            \n",
    "            # Check for the next page\n",
    "            if 'link' not in response.headers or 'rel=\"next\"' not in response.headers['link']:\n",
    "                break\n",
    "            page += 1\n",
    "            \n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "            \n",
    "    num_paths = len(all_file_paths)\n",
    "    \n",
    "    if num_paths == 0:\n",
    "        return 0, 0.0, 0\n",
    "    \n",
    "    # Calculate average and max path length\n",
    "    avg_path_length = sum(all_file_paths) / num_paths\n",
    "    max_path_length = max(all_file_paths)\n",
    "    \n",
    "    return num_paths, avg_path_length, max_path_length\n",
    "\n",
    "# ============================================================\n",
    "# Main Function: Pull Request Metrics\n",
    "# ============================================================\n",
    "def get_pull_request_metrics(owner: str, repo: str, pr_number: int, github_token: Optional[str] = None) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves the lines added, lines deleted, and the number of files changed\n",
    "    for a specific GitHub Pull Request.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. API URL for a specific Pull Request\n",
    "    pr_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}\"\n",
    "    \n",
    "    headers = {\n",
    "        # Standard Accept header for the V3 API\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch the Pull Request object\n",
    "        response = safe_request(\"GET\", pr_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        pr_data = response.json()\n",
    "\n",
    "        # 1. Line/File Change Metrics (from previous step)\n",
    "        num_additions = pr_data.get('additions', 0)\n",
    "        num_deletions = pr_data.get('deletions', 0)\n",
    "        num_files_changed = pr_data.get('changed_files', 0)\n",
    "        \n",
    "        # 2. NumCommits, NumComments (exclude review) \n",
    "        num_commits = pr_data.get('commits', 0)\n",
    "        num_comments = pr_data.get('comments', 0)\n",
    "        num_formal_reviews = get_review_count(owner, repo, pr_number, headers)\n",
    "        num_inline_comments = pr_data.get('review_comments', 0)\n",
    "        \n",
    "        #3\n",
    "        num_paths, avg_path_len, max_path_len = get_file_path_metrics(\n",
    "            owner, repo, pr_number, headers\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"Repo\": f\"{owner}/{repo}\",\n",
    "            \"PR_ID\": pr_number,            \n",
    "            \"Additions\": num_additions,\n",
    "            \"Deletions\": num_deletions,\n",
    "            \"Files_Changed\": num_files_changed,\n",
    "            \"NumCommits\": num_commits,\n",
    "            \"NumComments\": num_comments,\n",
    "            \"NumFormalReviews\": num_formal_reviews, \n",
    "            \"NumInlineComments\": num_inline_comments, \n",
    "            \"NumPathsInFile\": num_paths,          # The number of paths (or files changed)\n",
    "            \"AvgPathCharLength\": avg_path_len,    # Average characters in file paths\n",
    "            \"MaxPathCharLength\": max_path_len,    # Max characters in file paths\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for PR #{pr_number} in {owner}/{repo}: {e}\")\n",
    "            return None\n",
    "        \n",
    "# ============================================================\n",
    "# Main Helper Function: Pull Request Metrics (With Caching)\n",
    "# ============================================================\n",
    "def fetch_metrics(pr_list: list, github_token: Optional[str], cached_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches Pull Request metrics with caching and incremental saving.\n",
    "    The cache key is (owner, repo, pr_number).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Load Cache\n",
    "    try:\n",
    "        cached_df = pd.read_csv(cached_filename)\n",
    "        cached_df['Key'] = cached_df.apply(lambda row: (row['owner'], row['repo'], row['pr_number']), axis=1)\n",
    "        cached_keys = set(cached_df['Key'].tolist())\n",
    "        print(f\"Loaded {len(cached_keys)} existing metrics from {cached_filename}.\")\n",
    "        # Use a list for new results to easily append\n",
    "        new_results = []\n",
    "    except FileNotFoundError:\n",
    "        cached_df = pd.DataFrame()\n",
    "        cached_keys = set()\n",
    "        print(f\"No existing cache file found at {cached_filename}. Starting from scratch.\")\n",
    "        new_results = []\n",
    "\n",
    "    # Filter list to only process uncached entries. Input is (owner, repo, pr_number) tuples\n",
    "    uncached_prs = [\n",
    "        (owner, repo, pr_number) for owner, repo, pr_number in pr_list \n",
    "        if (owner, repo, pr_number) not in cached_keys\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total PRs to process: {len(pr_list)}. Uncached PRs remaining: {len(uncached_prs)}.\")\n",
    "\n",
    "    # Process uncached entries\n",
    "    for i, (owner, repo, pr_number) in enumerate(uncached_prs):\n",
    "        # Progress printout\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing PR {i+1}/{len(uncached_prs)}: {owner}/{repo} #{pr_number}\")\n",
    "        \n",
    "        # Call the core metric function\n",
    "        metrics = get_pull_request_metrics(owner, repo, pr_number, github_token)\n",
    "        \n",
    "        if metrics:\n",
    "            # Add key columns to the results dictionary\n",
    "            metrics['owner'] = owner\n",
    "            metrics['repo'] = repo\n",
    "            metrics['pr_number'] = pr_number\n",
    "            new_results.append(metrics)\n",
    "        \n",
    "        # Intermediate Save every 20 successful API calls (good practice for rate limit recovery)\n",
    "        if (len(new_results) > 0) and (len(new_results) % 20 == 0):\n",
    "            print(f\"--- Saving intermediate progress: {len(new_results)} new entries...\")\n",
    "            \n",
    "            # Combine new results with cached data and save\n",
    "            temp_df = pd.DataFrame(new_results)\n",
    "            # Ensure proper concatenation by dropping the temporary 'Key' column from cache\n",
    "            updated_df = pd.concat([cached_df.drop(columns=['Key']), temp_df], ignore_index=True)\n",
    "            updated_df = updated_df.drop_duplicates(subset=['owner', 'repo', 'pr_number'], keep='last') \n",
    "            updated_df.to_csv(cached_filename, index=False)\n",
    "            \n",
    "            # Update the cached_df and cached_keys for the next iteration (important for a true resume)\n",
    "            cached_df = updated_df\n",
    "            cached_df['Key'] = cached_df.apply(lambda row: (row['owner'], row['repo'], row['pr_number']), axis=1)\n",
    "            new_results = [] # Clear new results list as they are now in the cache file\n",
    "\n",
    "    # Final Save (if there are any remaining new results)\n",
    "    if new_results:\n",
    "        print(f\"--- Final save: {len(new_results)} remaining new entries...\")\n",
    "        temp_df = pd.DataFrame(new_results)\n",
    "        # Handle case where cached_df might be empty initially\n",
    "        if not cached_df.empty:\n",
    "            updated_df = pd.concat([cached_df.drop(columns=['Key']), temp_df], ignore_index=True)\n",
    "        else:\n",
    "            updated_df = temp_df\n",
    "    else:\n",
    "        updated_df = cached_df.drop(columns=['Key']) if not cached_df.empty else pd.DataFrame()\n",
    "\n",
    "    updated_df = updated_df.drop_duplicates(subset=['owner', 'repo', 'pr_number'], keep='last') \n",
    "    updated_df.to_csv(cached_filename, index=False)\n",
    "    print(f\"Metrics saved to {cached_filename}. Total entries: {len(updated_df)}.\")\n",
    "    \n",
    "    # 5. Return the final full DataFrame (cached + new)\n",
    "    return updated_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "pr_metrics_df_accept = fetch_metrics(ACCEPTED_PULL_REQUEST, GITHUB_TOKEN, \"pr_metrics_accepted_cached.csv\")\n",
    "pr_metrics_df_reject = fetch_metrics(REJECTED_PULL_REQUEST, GITHUB_TOKEN, \"pr_metrics_rejected_cached.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85794d",
   "metadata": {},
   "source": [
    "# 4. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n",
      "\n",
      "Accepted Repository Metrics DataFrame Created:\n",
      "Total rows in Accepted DataFrame: 43\n",
      "\n",
      "--- Processing Rejected Repositories ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments',\\n       'Formal_Review', 'Inline_Comments_Review', 'NumPathsInFile',\\n       'AvgPathCharLength', 'MaxPathCharLength'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# --- Processing Rejected Repositories ---\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing Rejected Repositories ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m final_df_reject \u001b[38;5;241m=\u001b[39m finalize_dataframe(\n\u001b[0;32m     53\u001b[0m     pr_metrics_df_reject, \n\u001b[0;32m     54\u001b[0m     repo_df, \n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr_metrics_rejected.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Save to a separate file\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRejected Repository Metrics DataFrame Created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows in Rejected DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_df_reject)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mfinalize_dataframe\u001b[1;34m(metrics_df, repo_df, output_filename)\u001b[0m\n\u001b[0;32m     21\u001b[0m column_order \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdditions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeletions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFiles_Changed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormal_Review\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInline_Comments_Review\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumPathsInFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgPathCharLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxPathCharLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the final column order\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df[column_order]\n\u001b[0;32m     28\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 4. Save the file (using CSV as per your original request)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments',\\n       'Formal_Review', 'Inline_Comments_Review', 'NumPathsInFile',\\n       'AvgPathCharLength', 'MaxPathCharLength'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'PR_ID': 'PR_Number',\n",
    "        'NumCommits': 'Commits', \n",
    "        'NumComments': 'Comments', \n",
    "        'NumFormalReviews': 'Formal_Review', \n",
    "        'NumInlineComments': 'Inline_Comments_Review'\n",
    "    }\n",
    "    final_df = metrics_df.rename(columns=rename_map)\n",
    "\n",
    "    # 3. Define the final column order\n",
    "    column_order = [\n",
    "        'Repo', 'PR_Number', 'Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments', 'Formal_Review', \n",
    "        'Inline_Comments_Review', 'NumPathsInFile', 'AvgPathCharLength', 'MaxPathCharLength', \n",
    "    ]\n",
    "    \n",
    "    # Apply the final column order\n",
    "    final_df = final_df[column_order]\n",
    "    final_df = final_df.fillna(0.0)\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    pr_metrics_df_accept, \n",
    "    \"pr_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    pr_metrics_df_reject, \n",
    "    \"pr_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
