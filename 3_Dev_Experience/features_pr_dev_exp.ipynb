{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da340c98",
   "metadata": {},
   "source": [
    "# Import the Hao-Li AIDev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2df0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df6a28",
   "metadata": {},
   "source": [
    "# 1. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89954997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the repository data for 'Java' language\n",
    "java_repo_df = repo_df[repo_df['language'] == 'Java'].copy()\n",
    "java_repo_select_df = java_repo_df[['id', 'full_name']]\n",
    "\n",
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "    java_repo_select_df,\n",
    "    left_on='repo_id',\n",
    "    right_on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# clean up extra attribute\n",
    "merged_pr_df = merged_pr_df.drop(columns=['id_y'])\n",
    "merged_pr_df = merged_pr_df.rename(columns={'id_x':'id'})\n",
    "\n",
    "# Filter PRs that were rejected (not merged) and create a new attribute\n",
    "accepted_prs = merged_pr_df[merged_pr_df['merged_at'].notnull()]\n",
    "rejected_prs = merged_pr_df[merged_pr_df['merged_at'].isnull()]\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "accepted_prs = accepted_prs[['full_name', 'number','user']]\n",
    "rejected_prs = rejected_prs[['full_name', 'number','user']]\n",
    "\n",
    "# print to csv for checking\n",
    "accepted_prs.to_csv(\"accepted_PR_dev.csv\", index=False)\n",
    "rejected_prs.to_csv(\"rejected_PR_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13051",
   "metadata": {},
   "source": [
    "## 1.1. Split the full_name of repo into owner and repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14daec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dotCMS', 'core', 32609, 'spbolton'), ('apache', 'pulsar', 24542, 'Apurva007'), ('microsoft', 'ApplicationInsights-Java', 4293, 'Copilot'), ('microsoft', 'typespec', 7783, 'Copilot'), ('valkey-io', 'valkey-glide', 4262, 'Copilot')]\n",
      "[('dotCMS', 'core', 32656, 'spbolton'), ('hyperledger', 'besu', 8904, 'jflo'), ('MeteorDevelopment', 'meteor-client', 5568, 'claudecodebyia'), ('apache', 'pulsar', 24145, 'alexander-nailed-it'), ('microsoft', 'typespec', 7651, 'Copilot')]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(pr_df):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Split the 'full_name' column into 'owner' and 'repo' columns\n",
    "    split_df = pr_df['full_name'].str.split('/', n=1, expand=True)\n",
    "    split_df.columns = ['owner', 'repo']\n",
    "    \n",
    "    # 2. Combine the split columns and the 'number' column into a list of tuples\n",
    "    combined_df = pd.concat([split_df, pr_df['number'], pr_df['user']], axis=1) # use axis=1 to apply the tuple creation row-wise across the three columns\n",
    "    unique_df = combined_df.drop_duplicates(subset=['owner', 'repo', 'user'])   # Drop duplicates based on the three columns\n",
    "    \n",
    "    # \n",
    "    repositories = unique_df.apply(tuple, axis=1).tolist()\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_PULL_REQUEST = process_repositories(accepted_prs)\n",
    "REJECTED_PULL_REQUEST = process_repositories(rejected_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5482fa",
   "metadata": {},
   "source": [
    "# 2. Helper code block to limit the API rate request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "def safe_request(method, url, headers=None, params=None, timeout=10, sleep_between=0.4):\n",
    "    \"\"\"\n",
    "    A rate-limit-safe GitHub request wrapper that handles:\n",
    "    - Primary rate limits (5000/hour)\n",
    "    - Secondary abuse limits (burst protection)\n",
    "    - GET and HEAD requests\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = requests.request(method, url, headers=headers, params=params, timeout=timeout)\n",
    "\n",
    "        # ============================================================\n",
    "        # Rate Limit handling\n",
    "        # ============================================================\n",
    "        # Primary rate limit\n",
    "        remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 1))\n",
    "        reset_ts = int(response.headers.get(\"X-RateLimit-Reset\", time.time()))\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait = max(reset_ts - int(time.time()), 10)\n",
    "            print(f\"[Primary Limit] Waiting {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # Secondary rate limit (abuse detection)\n",
    "        if response.status_code == 403:\n",
    "            print(\"[Secondary Limit] Hit GitHub abuse limit. Backing off 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        \n",
    "        # ============================================================\n",
    "        # API cache\n",
    "        # ============================================================\n",
    "        # Check if the response came from the cache\n",
    "        if hasattr(response, 'from_cache') and response.from_cache:\n",
    "            print(f\"[CACHE] Hit for {url}\")\n",
    "            # Skip the time.sleep(sleep_between) if it came from the cache\n",
    "            return response\n",
    "\n",
    "        # ============================================================\n",
    "        # Network handling\n",
    "        # ============================================================\n",
    "        # Success or other errors handled normally\n",
    "        if not response.ok:\n",
    "            response.raise_for_status()\n",
    "\n",
    "        # Small delay prevents triggering secondary limit\n",
    "        time.sleep(sleep_between)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf896a",
   "metadata": {},
   "source": [
    "# 3. Git API to extract metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n",
      "Error processing data for Copilot in microsoft/ApplicationInsights-Java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2FApplicationInsights-Java+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in microsoft/typespec: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2Ftypespec+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in valkey-io/valkey-glide: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Avalkey-io%2Fvalkey-glide+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in Azure/azure-sdk-for-java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AAzure%2Fazure-sdk-for-java+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in camunda/camunda: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Acamunda%2Fcamunda+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 43 seconds...\n",
      "Error processing data for Copilot in EduMIPS64/edumips64: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AEduMIPS64%2Fedumips64+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in apolloconfig/apollo: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aapolloconfig%2Fapollo+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in objectionary/eo: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aobjectionary%2Feo+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in 1c-syntax/bsl-language-server: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3A1c-syntax%2Fbsl-language-server+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in wgzhao/Addax: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Awgzhao%2FAddax+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in OWASP/wrongsecrets: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AOWASP%2Fwrongsecrets+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in W1LDN16H7/JPL: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AW1LDN16H7%2FJPL+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in binarywang/WxJava: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Abinarywang%2FWxJava+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in wgzhao/dbeaver-agent: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Awgzhao%2Fdbeaver-agent+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 46 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 44 seconds...\n",
      "[Primary Limit] Waiting 43 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 43 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 44 seconds...\n",
      "[Primary Limit] Waiting 3117 seconds...\n",
      "[Primary Limit] Waiting 43 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "Error processing data for Copilot in microsoft/typespec: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2Ftypespec+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 125 seconds...\n",
      "Error processing data for Copilot in camunda/camunda: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Acamunda%2Fcamunda+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in valkey-io/valkey-glide: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Avalkey-io%2Fvalkey-glide+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in Azure/azure-sdk-for-java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AAzure%2Fazure-sdk-for-java+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in microsoft/ApplicationInsights-Java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2FApplicationInsights-Java+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in akto-api-security/akto: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aakto-api-security%2Fakto+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in RyanSusana/elepy: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3ARyanSusana%2Felepy+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in trinodb/trino: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Atrinodb%2Ftrino+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in hyperxpro/Brotli4j: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Ahyperxpro%2FBrotli4j+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in YunaiV/ruoyi-vue-pro: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AYunaiV%2Fruoyi-vue-pro+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 46 seconds...\n",
      "Error processing data for Copilot in mixpanel/mixpanel-android: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amixpanel%2Fmixpanel-android+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in EduMIPS64/edumips64: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AEduMIPS64%2Fedumips64+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in 1c-syntax/bsl-language-server: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3A1c-syntax%2Fbsl-language-server+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in binarywang/WxJava: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Abinarywang%2FWxJava+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in AutoMQ/automq: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AAutoMQ%2Fautomq+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in jenkinsci/jira-plugin: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Ajenkinsci%2Fjira-plugin+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in objectionary/eo: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aobjectionary%2Feo+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in halo-dev/halo: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Ahalo-dev%2Fhalo+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in qaiu/netdisk-fast-download: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aqaiu%2Fnetdisk-fast-download+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 44 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "Error processing data for Copilot in wso2/product-is: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Awso2%2Fproduct-is+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in CarGuo/GSYVideoPlayer: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3ACarGuo%2FGSYVideoPlayer+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in apolloconfig/apollo: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Aapolloconfig%2Fapollo+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in OWASP/wrongsecrets: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AOWASP%2Fwrongsecrets+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in microsoft/HydraLab: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2FHydraLab+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in yegor256/cactoos: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Ayegor256%2Fcactoos+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in microsoft/semantic-kernel-java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Amicrosoft%2Fsemantic-kernel-java+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in vaadin/flow: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Avaadin%2Fflow+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in feiyuchuixue/sz-boot-parent: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Afeiyuchuixue%2Fsz-boot-parent+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "Error processing data for Copilot in tonihele/OpenKeeper: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Atonihele%2FOpenKeeper+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in W1LDN16H7/JPL: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AW1LDN16H7%2FJPL+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in geoserver/geoserver: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3Ageoserver%2Fgeoserver+is%3Apr+author%3ACopilot&per_page=1\n",
      "Error processing data for Copilot in Azure-Samples/azure-search-openai-demo-java: 422 Client Error: Unprocessable Entity for url: https://api.github.com/search/issues?q=repo%3AAzure-Samples%2Fazure-search-openai-demo-java+is%3Apr+author%3ACopilot&per_page=1\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 41 seconds...\n",
      "[Primary Limit] Waiting 43 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 3189 seconds...\n",
      "[Primary Limit] Waiting 10 seconds...\n",
      "[Primary Limit] Waiting 46 seconds...\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 45 seconds...\n",
      "[Primary Limit] Waiting 46 seconds...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper 0: Total Commit Counts for that PR\n",
    "# ============================================================\n",
    "def get_commit_count(owner, repo, headers):\n",
    "    \"\"\"Retrieves the total number of commits for the entire repository.\"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    params = {\"per_page\": 1}\n",
    "    \n",
    "    try:\n",
    "        # Use HEAD request and pagination trick for total count\n",
    "        response = safe_request(\"HEAD\", url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        link_header = response.headers.get('Link')\n",
    "\n",
    "        if link_header:\n",
    "            last_page_match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if last_page_match:\n",
    "                return int(last_page_match.group(1))\n",
    "        \n",
    "        # Fallback for very small repos\n",
    "        return 0 \n",
    "        \n",
    "    except requests.exceptions.RequestException:\n",
    "        return 0\n",
    "    \n",
    "# ============================================================\n",
    "# Helper 1: Developer PR Count for that Repo\n",
    "# ============================================================\n",
    "def get_developer_pr_count(owner: str, repo: str, developer_login: str, headers: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the total number of PRs submitted by the developer in the repo \n",
    "    using the GitHub Search API (which correctly handles author filtering).\n",
    "    \"\"\"\n",
    "    # Use the Search Issues endpoint, which allows filtering by author and type:pr\n",
    "    url = \"https://api.github.com/search/issues\"\n",
    "    \n",
    "    # Construct the query: repo:owner/repo is:pr author:developer_login\n",
    "    query = f\"repo:{owner}/{repo} is:pr author:{developer_login}\"\n",
    "    \n",
    "    # We only need the total_count, so we request a minimal response (per_page=1)\n",
    "    params = {\"q\": query, \"per_page\": 1}\n",
    "    \n",
    "    response = safe_request(\"GET\", url, headers=headers, params=params)\n",
    "    if response and response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            # The search API returns the total count directly in the response body\n",
    "            return data.get('total_count', 0)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"   [Error] Could not decode JSON response from Search API.\")\n",
    "            return 0\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper 2: Followers and Account Age (Active Duration)\n",
    "# ============================================================\n",
    "def get_developer_social_metrics(developer_login: str, headers: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetches the number of followers and the account creation date.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/users/{developer_login}\"\n",
    "    \n",
    "    response = safe_request(\"GET\", url, headers=headers)\n",
    "    if response and response.status_code == 200:\n",
    "        user_data = response.json()\n",
    "        \n",
    "        # Calculate Active Duration\n",
    "        created_at_str = user_data.get('created_at')\n",
    "        account_age_days = 0\n",
    "        if created_at_str:\n",
    "            created_date = datetime.strptime(created_at_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            account_age_days = (datetime.now() - created_date).days\n",
    "            \n",
    "        return {\n",
    "            \"Followers\": user_data.get('followers', 0),\n",
    "            \"Account_Created_At\": created_at_str,\n",
    "            \"Account_Active_Duration_Days\": account_age_days\n",
    "        }\n",
    "    return {\n",
    "        \"Followers\": 0,\n",
    "        \"Account_Created_At\": None,\n",
    "        \"Account_Active_Duration_Days\": 0\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Helper 3: Global Developer PR Count (Across all Repos)\n",
    "# ============================================================\n",
    "def get_developer_global_pr_count(developer_login: str, headers: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the total number of PRs submitted by the developer globally \n",
    "    using the GitHub Search API without the repository filter.\n",
    "    \"\"\"\n",
    "    url = \"https://api.github.com/search/issues\"\n",
    "    \n",
    "    # Construct the query: is:pr author:developer_login (No repo filter)\n",
    "    query = f\"is:pr author:{developer_login}\"\n",
    "    \n",
    "    params = {\"q\": query, \"per_page\": 1}\n",
    "    \n",
    "    response = safe_request(\"GET\", url, headers=headers, params=params)\n",
    "    if response and response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            return data.get('total_count', 0)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"   [Error] Could not decode JSON response from Global Search API.\")\n",
    "            return 0\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper Total: All the Developer Characteristics \n",
    "# ============================================================\n",
    "def get_developer_characteristic_metrics(owner: str, repo: str, pr_number, developer_login: str, github_token: Optional[str] = None) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves a developer's total commits and their percentage contribution to the repo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup headers with token\n",
    "    headers = {}\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Developer's total commits in the repo (using the same pagination trick)\n",
    "        developer_commit_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "        params = {\"per_page\": 1, \"author\": developer_login}\n",
    "        dev_response = safe_request(\"HEAD\", developer_commit_url, headers=headers, params=params)\n",
    "        \n",
    "        developer_commits = 0\n",
    "        if dev_response and dev_response.status_code == 200:\n",
    "            dev_link_header = dev_response.headers.get('Link')\n",
    "            if dev_link_header:\n",
    "                dev_last_page_match = re.search(r'page=(\\d+)>; rel=\"last\"', dev_link_header)\n",
    "                if dev_last_page_match:\n",
    "                    developer_commits = int(dev_last_page_match.group(1))\n",
    "\n",
    "        # 2. Total commits for the entire repository\n",
    "        total_commits = get_commit_count(owner, repo, headers)\n",
    "        \n",
    "        # 3. Calculate the percentage contribution\n",
    "        commit_percentage = (developer_commits / total_commits) * 100 if total_commits > 0 else 0.0\n",
    "        \n",
    "        # 4. Total PRs submitted by the developer \n",
    "        total_prs_submitted = get_developer_pr_count(owner, repo, developer_login, headers)\n",
    "        \n",
    "        # 5. Social and Account age metrics \n",
    "        social_metrics = get_developer_social_metrics(developer_login, headers)\n",
    "        \n",
    "        # 6. Total PRs submitted by the developer (Globally)\n",
    "        total_prs_global = get_developer_global_pr_count(developer_login, headers)\n",
    "\n",
    "        return {\n",
    "            \"Repo\": f\"{owner}/{repo}\",\n",
    "            \"PR_ID\": pr_number,\n",
    "            \"Developer_Login\": developer_login,\n",
    "            \"Total_Repo_Commits\": total_commits,\n",
    "            \"Developer_Commits\": developer_commits,\n",
    "            \"Developer_Commit_Percentage\": round(commit_percentage, 2),\n",
    "            \"Total_PRs_Submitted\": total_prs_submitted, \n",
    "            \"Total_PRs_Global\": total_prs_global,        \n",
    "            **social_metrics\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for {developer_login} in {owner}/{repo}: {e}\")\n",
    "        return None\n",
    "        \n",
    "# ============================================================\n",
    "# Main Helper Function: Developer Experience Metrics (With Caching)\n",
    "# ============================================================\n",
    "def fetch_metrics(pr_list: list, github_token: Optional[str], cached_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches Developer Experience metrics with caching and incremental saving.\n",
    "    The cache key is (owner, repo, pr_number, developer_login).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 1. Load Cache\n",
    "    try:\n",
    "        cached_df = pd.read_csv(cached_filename)\n",
    "        # Create a unique key for caching/comparison\n",
    "        cached_df['Key'] = cached_df.apply(lambda row: (row['owner'], row['repo'], row['pr_number'], row['developer_login']), axis=1)\n",
    "        cached_keys = set(cached_df['Key'].tolist())\n",
    "        print(f\"Loaded {len(cached_keys)} existing metrics from {cached_filename}.\")\n",
    "        new_results = []\n",
    "    except FileNotFoundError:\n",
    "        cached_df = pd.DataFrame()\n",
    "        cached_keys = set()\n",
    "        print(f\"No existing cache file found at {cached_filename}. Starting from scratch.\")\n",
    "        new_results = []\n",
    "\n",
    "    # 2. Filter list to only process uncached entries\n",
    "    # The input list is (owner, repo, pr_number, developer_login) tuples\n",
    "    uncached_prs = [\n",
    "        (owner, repo, pr_number, developer_login) \n",
    "        for owner, repo, pr_number, developer_login in pr_list \n",
    "        if (owner, repo, pr_number, developer_login) not in cached_keys\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total PRs to process: {len(pr_list)}. Uncached PRs remaining: {len(uncached_prs)}.\")\n",
    "\n",
    "    # 3. Process uncached entries\n",
    "    for i, (owner, repo, pr_number, developer_login) in enumerate(uncached_prs):\n",
    "        # Progress printout\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing PR {i+1}/{len(uncached_prs)}: {owner}/{repo} #{pr_number} by {developer_login}\")\n",
    "        \n",
    "        # Call the core metric function\n",
    "        metrics = get_developer_characteristic_metrics(owner, repo, pr_number, developer_login, github_token)\n",
    "        \n",
    "        if metrics:\n",
    "            # Add key columns to the results dictionary\n",
    "            metrics['owner'] = owner\n",
    "            metrics['repo'] = repo\n",
    "            metrics['pr_number'] = pr_number\n",
    "            metrics['developer_login'] = developer_login\n",
    "            new_results.append(metrics)\n",
    "        \n",
    "        # Intermediate Save every 20 successful API calls (good practice for rate limit recovery)\n",
    "        if (len(new_results) > 0) and (len(new_results) % 20 == 0):\n",
    "            print(f\"--- Saving intermediate progress: {len(new_results)} new entries...\")\n",
    "            \n",
    "            # Combine new results with cached data and save\n",
    "            temp_df = pd.DataFrame(new_results)\n",
    "            updated_df = pd.concat([cached_df.drop(columns=['Key']), temp_df], ignore_index=True)\n",
    "            subset_cols = ['owner', 'repo', 'pr_number', 'developer_login']\n",
    "            updated_df = updated_df.drop_duplicates(subset=subset_cols, keep='last') \n",
    "            updated_df.to_csv(cached_filename, index=False)\n",
    "            \n",
    "            # Update the cached_df and cached_keys for the next iteration\n",
    "            cached_df = updated_df\n",
    "            cached_df['Key'] = cached_df.apply(lambda row: (row['owner'], row['repo'], row['pr_number'], row['developer_login']), axis=1)\n",
    "            new_results = [] # Clear new results list\n",
    "\n",
    "    # 4. Final Save\n",
    "    if new_results:\n",
    "        print(f\"--- Final save: {len(new_results)} remaining new entries...\")\n",
    "        temp_df = pd.DataFrame(new_results)\n",
    "        if not cached_df.empty:\n",
    "            updated_df = pd.concat([cached_df.drop(columns=['Key']), temp_df], ignore_index=True)\n",
    "        else:\n",
    "            updated_df = temp_df\n",
    "    else:\n",
    "        updated_df = cached_df.drop(columns=['Key']) if not cached_df.empty else pd.DataFrame()\n",
    "\n",
    "    subset_cols = ['owner', 'repo', 'pr_number', 'developer_login']\n",
    "    updated_df = updated_df.drop_duplicates(subset=subset_cols, keep='last') \n",
    "    updated_df.to_csv(cached_filename, index=False)\n",
    "    print(f\"Metrics saved to {cached_filename}. Total entries: {len(updated_df)}.\")\n",
    "    \n",
    "    # 5. Return the final full DataFrame\n",
    "    return updated_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "pr_dev_df_accept = fetch_metrics(ACCEPTED_PULL_REQUEST, GITHUB_TOKEN, \"pr_dev_metrics_accepted_cached.csv\")\n",
    "pr_dev_df_reject = fetch_metrics(REJECTED_PULL_REQUEST, GITHUB_TOKEN, \"pr_dev_metrics_rejected_cached.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd87964",
   "metadata": {},
   "source": [
    "# 4. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n",
      "\n",
      "Accepted Repository Metrics DataFrame Created:\n",
      "Total rows in Accepted DataFrame: 36\n",
      "\n",
      "--- Processing Rejected Repositories ---\n",
      "\n",
      "Rejected Repository Metrics DataFrame Created:\n",
      "Total rows in Rejected DataFrame: 45\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'PR_ID': 'PR_Number',\n",
    "        'Developer_Login': 'User',\n",
    "        'Total_Repo_Commits': 'Total_Commits_Repo', \n",
    "        'Developer_Commits': 'Dev_Commits_Repo', \n",
    "        'Developer_Commit_Percentage': 'Dev_Commits_Repo_Percentage', \n",
    "        'Total_PRs_Submitted': 'Dev_PR_Total', \n",
    "        'Account_Created_At': 'Creation_Date', \n",
    "        'Account_Active_Duration_Days': 'Days_Active', \n",
    "    }\n",
    "    final_df = metrics_df.rename(columns=rename_map)\n",
    "\n",
    "    # Apply the final column order\n",
    "    final_df = final_df.fillna(0.0)\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    pr_dev_df_accept, \n",
    "    \"pr_dev_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    pr_dev_df_reject, \n",
    "    \"pr_dev_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
