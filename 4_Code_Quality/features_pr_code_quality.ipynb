{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da340c98",
   "metadata": {},
   "source": [
    "# Import the Hao-Li AIDev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2df0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df6a28",
   "metadata": {},
   "source": [
    "# 1. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89954997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the repository data for 'Java' language\n",
    "java_repo_df = repo_df[repo_df['language'] == 'Java'].copy()\n",
    "java_repo_select_df = java_repo_df[['id', 'full_name']]\n",
    "\n",
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "    java_repo_select_df,\n",
    "    left_on='repo_id',\n",
    "    right_on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# clean up extra attribute\n",
    "merged_pr_df = merged_pr_df.drop(columns=['id_y'])\n",
    "merged_pr_df = merged_pr_df.rename(columns={'id_x':'id'})\n",
    "\n",
    "# Filter PRs that were rejected (not merged) and create a new attribute\n",
    "accepted_prs = merged_pr_df[merged_pr_df['merged_at'].notnull()]\n",
    "rejected_prs = merged_pr_df[merged_pr_df['merged_at'].isnull()]\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "accepted_prs = accepted_prs[['full_name', 'number']]\n",
    "rejected_prs = rejected_prs[['full_name', 'number']]\n",
    "\n",
    "# print to csv for checking\n",
    "accepted_prs.to_csv(\"accepted_PR.csv\", index=False)\n",
    "rejected_prs.to_csv(\"rejected_PR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13051",
   "metadata": {},
   "source": [
    "## 1.1. Split the full_name of repo into owner and repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14daec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dotCMS', 'core', 32609), ('apache', 'pulsar', 24542), ('dotCMS', 'core', 32771), ('dotCMS', 'core', 32561), ('microsoft', 'ApplicationInsights-Java', 4293)]\n",
      "[('dotCMS', 'core', 32656), ('dotCMS', 'core', 32657), ('dotCMS', 'core', 32658), ('dotCMS', 'core', 32659), ('dotCMS', 'core', 32660)]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(pr_df):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Split the 'full_name' column into 'owner' and 'repo' columns\n",
    "    split_df = pr_df['full_name'].str.split('/', n=1, expand=True)\n",
    "    split_df.columns = ['owner', 'repo']\n",
    "    \n",
    "    # 2. Combine the split columns and the 'number' column into a list of tuples\n",
    "    # We use axis=1 to apply the tuple creation row-wise across the three columns\n",
    "    repositories = pd.concat([split_df, pr_df['number']], axis=1).apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    # Print the first 5 results for verification\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_PULL_REQUEST = process_repositories(accepted_prs)\n",
    "REJECTED_PULL_REQUEST = process_repositories(rejected_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5482fa",
   "metadata": {},
   "source": [
    "# 2. Helper code block to limit the API rate request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def safe_request(method, url, headers=None, params=None, timeout=10, sleep_between=0.4):\n",
    "    \"\"\"\n",
    "    A rate-limit-safe GitHub request wrapper that handles:\n",
    "    - Primary rate limits (5000/hour)\n",
    "    - Secondary abuse limits (burst protection)\n",
    "    - GET and HEAD requests\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = requests.request(method, url, headers=headers, params=params, timeout=timeout)\n",
    "\n",
    "        # Primary rate limit\n",
    "        remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 1))\n",
    "        reset_ts = int(response.headers.get(\"X-RateLimit-Reset\", time.time()))\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait = max(reset_ts - int(time.time()), 10)\n",
    "            print(f\"[Primary Limit] Waiting {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # Secondary rate limit (abuse detection)\n",
    "        if response.status_code == 403:\n",
    "            print(\"[Secondary Limit] Hit GitHub abuse limit. Backing off 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        # Success or other errors handled normally\n",
    "        if not response.ok:\n",
    "            response.raise_for_status()\n",
    "\n",
    "        # Small delay prevents triggering secondary limit\n",
    "        time.sleep(sleep_between)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf896a",
   "metadata": {},
   "source": [
    "# 3. Git API to extract metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d928623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n",
      "Fetching file details for PR #32609 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 9\n",
      "Fetching file details for PR #24542 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 4\n",
      "Fetching file details for PR #32656 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 25\n",
      "Fetching file details for PR #32657 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 18\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Get the files name, patch code, addition, deletion, status, and RAW URL\n",
    "# ============================================================\n",
    "def get_pr_file_details(owner: str, repo: str, pr_number: int, github_token: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches the details for all files changed in a Pull Request,\n",
    "    INCLUDING the raw URL for the file content.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
    "    all_file_details = []\n",
    "    page = 1\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    }\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {github_token}\"\n",
    "\n",
    "    print(f\"Fetching file details for PR #{pr_number} (Paginating 100 files/page)...\")\n",
    "\n",
    "    while True:\n",
    "        params = {\"per_page\": 100, \"page\": page}\n",
    "        \n",
    "        try:\n",
    "            response = safe_request(\"GET\", base_url, headers=headers, params=params,)\n",
    "            response.raise_for_status()\n",
    "            files_data = response.json()\n",
    "\n",
    "            if not files_data:\n",
    "                break\n",
    "\n",
    "            for file in files_data:\n",
    "                filename = file.get('filename')\n",
    "                patch_content = file.get('patch')\n",
    "                # final_patch = patch_content if patch_content else \"NULL\"\n",
    "                raw_url = file.get('raw_url') \n",
    "                \n",
    "                # All the file metrics here\n",
    "                all_file_details.append({\n",
    "                    \"filename\": filename,\n",
    "                    #\"patch\": final_patch,\n",
    "                    \"status\": file.get('status'),\n",
    "                    #\"additions\": file.get('additions', 0),\n",
    "                    #\"deletions\": file.get('deletions', 0),\n",
    "                    \"raw_url\": raw_url \n",
    "                })\n",
    "            \n",
    "            # Check for the next page header\n",
    "            if 'link' not in response.headers or 'rel=\"next\"' not in response.headers['link']:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during API call on page {page}: {e}\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Finished fetching. Total files processed: {len(all_file_details)}\")\n",
    "    return all_file_details\n",
    "        \n",
    "# ============================================================\n",
    "# Main Helper: Fetch the main metric functions \n",
    "# ============================================================\n",
    "def fetch_metrics(repo_list, token):\n",
    "    results = []\n",
    "    # limit the number of repositories processed here for testing REPOSITORIES[:10]:\n",
    "    for owner, repo, pr_number in repo_list[:2]: # Apply the test limit here\n",
    "        metrics = get_pr_file_details(owner, repo, pr_number, token)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    \n",
    "    # Create the Metric DataFrame\n",
    "    return results # Return a List[List[Dict]] to be process later\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "files_list_accepted = fetch_metrics(ACCEPTED_PULL_REQUEST, GITHUB_TOKEN)\n",
    "files_list_rejected = fetch_metrics(REJECTED_PULL_REQUEST, GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df53fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting filtering and sorting\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Filter and Aggregate PR Data\n",
    "# ============================================================\n",
    "def filter_and_aggregate_pr_data(pr_files_list: List[List[Dict]], repo_list: List[Tuple[str, str, int]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filters file details for Java files that are not deleted and aggregates the \n",
    "    relevant data (like raw_urls) at the Pull Request level.\n",
    "\n",
    "    Args:\n",
    "        pr_files_list: The nested list of file details from the GitHub API.\n",
    "        repo_list: The original list of (owner, repo, pr_number) tuples \n",
    "                   used to fetch the data.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, one for each PR, containing aggregated metrics.\n",
    "    \"\"\"\n",
    "    aggregated_pr_data = []\n",
    "    \n",
    "    # Iterate through the results for each PR\n",
    "    for pr_index, pr_files in enumerate(pr_files_list):\n",
    "        \n",
    "        # Safely retrieve metadata for the current PR\n",
    "        if pr_index >= len(repo_list):\n",
    "            print(f\"Warning: Missing metadata for PR at index {pr_index}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        owner, repo, pr_number = repo_list[pr_index]\n",
    "        \n",
    "        java_files_to_analyze = []\n",
    "        \n",
    "        # --- File-level filtering ---\n",
    "        for file in pr_files:\n",
    "            filename = file.get('filename', '')\n",
    "            status = file.get('status', '')\n",
    "            \n",
    "            # 1. Detect .java file in the file name (case-insensitive)\n",
    "            is_java = filename.lower().endswith('.java')\n",
    "            \n",
    "            # 2. Exclude status deleted (we only analyze added or modified code)\n",
    "            is_not_deleted = status != 'deleted'\n",
    "            \n",
    "            # Store the file name and raw URL for the non-deleted Java file\n",
    "            if is_java and is_not_deleted:\n",
    "                java_files_to_analyze.append({\n",
    "                    \"file_name\": filename,\n",
    "                    \"raw_url\": file.get('raw_url')\n",
    "                })\n",
    "        \n",
    "        # --- PR-level aggregation ---\n",
    "        aggregated_pr_data.append({\n",
    "            'owner': owner,\n",
    "            'repo': repo,\n",
    "            'pr_number': pr_number,\n",
    "            'java_files_analyzed_count': len(java_files_to_analyze),\n",
    "            'files_to_analyze': java_files_to_analyze, # List[Dict]\n",
    "            # add the PMD violation counts later\n",
    "            'pmd_violations': {} \n",
    "        })\n",
    "        \n",
    "    return aggregated_pr_data\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM: Filter the \n",
    "# ============================================================\n",
    "print(\"\\nStarting filtering and sorting\")\n",
    "pr_code_metrics_filtered_accepted = filter_and_aggregate_pr_data(files_list_accepted, ACCEPTED_PULL_REQUEST)\n",
    "pr_code_metrics_filtered_rejected = filter_and_aggregate_pr_data(files_list_accepted, ACCEPTED_PULL_REQUEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "befa5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing PR: dotCMS/core #32609 (0 files) ---\n",
      "\n",
      "--- Processing PR: apache/pulsar #24542 (4 files) ---\n"
     ]
    }
   ],
   "source": [
    "def download_file(raw_url, local_path, token=None):\n",
    "    \"\"\"Downloads a single file from GitHub's raw URL.\"\"\"\n",
    "    headers = {}\n",
    "    if token:\n",
    "        headers['Authorization'] = f\"token {token}\"\n",
    "        \n",
    "    response = requests.get(raw_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download {raw_url}. Status: {response.status_code}\")\n",
    "        return False\n",
    "        \n",
    "# Example of the download loop:\n",
    "for pr_data in pr_code_metrics_filtered_accepted:\n",
    "    owner = pr_data['owner']\n",
    "    repo = pr_data['repo']\n",
    "    pr_number = pr_data['pr_number']\n",
    "    files_to_analyze = pr_data['files_to_analyze'] \n",
    "    \n",
    "    # Create a local staging directory structure\n",
    "    # This prevents file path clashes and organizes your PMD reports\n",
    "    base_dir = \"./pr_analysis_staging\"\n",
    "    local_staging_dir = os.path.join(base_dir, owner, repo, str(pr_number))\n",
    "    \n",
    "    print(f\"\\n--- Processing PR: {owner}/{repo} #{pr_number} ({len(files_to_analyze)} files) ---\")\n",
    "    \n",
    "    # Inner loop: Iterate over the list of file dictionaries\n",
    "    for file_data in files_to_analyze:\n",
    "        \n",
    "        # Get the two keys you need from the file dictionary\n",
    "        file_name = file_data['file_name']\n",
    "        raw_url = file_data['raw_url']\n",
    "        \n",
    "        # Determine the full local path for this file\n",
    "        local_path = os.path.join(local_staging_dir, file_name)\n",
    "        \n",
    "        download_file(raw_url, local_path, GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n",
      "\n",
      "Accepted Repository Metrics DataFrame Created:\n",
      "Total rows in Accepted DataFrame: 43\n",
      "\n",
      "--- Processing Rejected Repositories ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments',\\n       'Formal_Review', 'Inline_Comments_Review', 'NumPathsInFile',\\n       'AvgPathCharLength', 'MaxPathCharLength'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# --- Processing Rejected Repositories ---\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing Rejected Repositories ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m final_df_reject \u001b[38;5;241m=\u001b[39m finalize_dataframe(\n\u001b[0;32m     53\u001b[0m     pr_metrics_df_reject, \n\u001b[0;32m     54\u001b[0m     repo_df, \n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr_metrics_rejected.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Save to a separate file\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRejected Repository Metrics DataFrame Created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows in Rejected DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_df_reject)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mfinalize_dataframe\u001b[1;34m(metrics_df, repo_df, output_filename)\u001b[0m\n\u001b[0;32m     21\u001b[0m column_order \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdditions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeletions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFiles_Changed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormal_Review\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInline_Comments_Review\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumPathsInFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvgPathCharLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxPathCharLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the final column order\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df[column_order]\n\u001b[0;32m     28\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 4. Save the file (using CSV as per your original request)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments',\\n       'Formal_Review', 'Inline_Comments_Review', 'NumPathsInFile',\\n       'AvgPathCharLength', 'MaxPathCharLength'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'PR_ID': 'PR_number',\n",
    "        'NumCommits': 'Commits', \n",
    "        'NumComments': 'Comments', \n",
    "        'NumFormalReviews': 'Formal_Review', \n",
    "        'NumInlineComments': 'Inline_Comments_Review'\n",
    "    }\n",
    "    final_df = metrics_df.rename(columns=rename_map)\n",
    "\n",
    "    # 3. Define the final column order\n",
    "    column_order = [\n",
    "        'Repo', 'PR_number', 'Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments', 'Formal_Review', \n",
    "        'Inline_Comments_Review', 'NumPathsInFile', 'AvgPathCharLength', 'MaxPathCharLength', \n",
    "    ]\n",
    "    \n",
    "    # Apply the final column order\n",
    "    final_df = final_df[column_order]\n",
    "    final_df = final_df.fillna(0.0)\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    pr_metrics_df_accept, \n",
    "    \"pr_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    pr_metrics_df_reject, \n",
    "    \"pr_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
