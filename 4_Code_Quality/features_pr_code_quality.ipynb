{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da340c98",
   "metadata": {},
   "source": [
    "# Import the Hao-Li AIDev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2df0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df6a28",
   "metadata": {},
   "source": [
    "# 1. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89954997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the repository data for 'Java' language\n",
    "java_repo_df = repo_df[repo_df['language'] == 'Java'].copy()\n",
    "java_repo_select_df = java_repo_df[['id', 'full_name']]\n",
    "\n",
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "    java_repo_select_df,\n",
    "    left_on='repo_id',\n",
    "    right_on='id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# clean up extra attribute\n",
    "merged_pr_df = merged_pr_df.drop(columns=['id_y'])\n",
    "merged_pr_df = merged_pr_df.rename(columns={'id_x':'id'})\n",
    "\n",
    "# Filter PRs that were rejected (not merged) and create a new attribute\n",
    "accepted_prs = merged_pr_df[merged_pr_df['merged_at'].notnull()]\n",
    "rejected_prs = merged_pr_df[merged_pr_df['merged_at'].isnull()]\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "accepted_prs = accepted_prs[['full_name', 'number']]\n",
    "rejected_prs = rejected_prs[['full_name', 'number']]\n",
    "\n",
    "# print to csv for checking\n",
    "accepted_prs.to_csv(\"accepted_PR.csv\", index=False)\n",
    "rejected_prs.to_csv(\"rejected_PR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b13051",
   "metadata": {},
   "source": [
    "## 1.1. Split the full_name of repo into owner and repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14daec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dotCMS', 'core', 32609), ('apache', 'pulsar', 24542), ('dotCMS', 'core', 32771), ('dotCMS', 'core', 32561), ('microsoft', 'ApplicationInsights-Java', 4293)]\n",
      "[('dotCMS', 'core', 32656), ('dotCMS', 'core', 32657), ('dotCMS', 'core', 32658), ('dotCMS', 'core', 32659), ('dotCMS', 'core', 32660)]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(pr_df):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Split the 'full_name' column into 'owner' and 'repo' columns\n",
    "    split_df = pr_df['full_name'].str.split('/', n=1, expand=True)\n",
    "    split_df.columns = ['owner', 'repo']\n",
    "    \n",
    "    # 2. Combine the split columns and the 'number' column into a list of tuples\n",
    "    # We use axis=1 to apply the tuple creation row-wise across the three columns\n",
    "    repositories = pd.concat([split_df, pr_df['number']], axis=1).apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    # Print the first 5 results for verification\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_PULL_REQUEST = process_repositories(accepted_prs)\n",
    "REJECTED_PULL_REQUEST = process_repositories(rejected_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5482fa",
   "metadata": {},
   "source": [
    "# 2. Helper code block to limit the API rate request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c42486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def safe_request(method, url, headers=None, params=None, timeout=10, sleep_between=0.4):\n",
    "    \"\"\"\n",
    "    A rate-limit-safe GitHub request wrapper that handles:\n",
    "    - Primary rate limits (5000/hour)\n",
    "    - Secondary abuse limits (burst protection)\n",
    "    - GET and HEAD requests\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = requests.request(method, url, headers=headers, params=params, timeout=timeout)\n",
    "\n",
    "        # Primary rate limit\n",
    "        remaining = int(response.headers.get(\"X-RateLimit-Remaining\", 1))\n",
    "        reset_ts = int(response.headers.get(\"X-RateLimit-Reset\", time.time()))\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait = max(reset_ts - int(time.time()), 10)\n",
    "            print(f\"[Primary Limit] Waiting {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # Secondary rate limit (abuse detection)\n",
    "        if response.status_code == 403:\n",
    "            print(\"[Secondary Limit] Hit GitHub abuse limit. Backing off 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "\n",
    "        # Success or other errors handled normally\n",
    "        if not response.ok:\n",
    "            response.raise_for_status()\n",
    "\n",
    "        # Small delay prevents triggering secondary limit\n",
    "        time.sleep(sleep_between)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf896a",
   "metadata": {},
   "source": [
    "# 3. Git API to extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06670a",
   "metadata": {},
   "source": [
    "## 3.1. API to extract git metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d928623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n",
      "Fetching file details for PR #32609 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 9\n",
      "Fetching file details for PR #24542 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 4\n",
      "Fetching file details for PR #32656 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 25\n",
      "Fetching file details for PR #32657 (Paginating 100 files/page)...\n",
      "Finished fetching. Total files processed: 18\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Get the files name, patch code, addition, deletion, status, and RAW URL\n",
    "# ============================================================\n",
    "def get_pr_file_details(owner: str, repo: str, pr_number: int, github_token: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches the details for all files changed in a Pull Request,\n",
    "    INCLUDING the raw URL for the file content.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n",
    "    all_file_details = []\n",
    "    page = 1\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    }\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {github_token}\"\n",
    "\n",
    "    print(f\"Fetching file details for PR #{pr_number} (Paginating 100 files/page)...\")\n",
    "\n",
    "    while True:\n",
    "        params = {\"per_page\": 100, \"page\": page}\n",
    "        \n",
    "        try:\n",
    "            response = safe_request(\"GET\", base_url, headers=headers, params=params,)\n",
    "            response.raise_for_status()\n",
    "            files_data = response.json()\n",
    "\n",
    "            if not files_data:\n",
    "                break\n",
    "\n",
    "            for file in files_data:\n",
    "                filename = file.get('filename')\n",
    "                patch_content = file.get('patch')\n",
    "                # final_patch = patch_content if patch_content else \"NULL\"\n",
    "                raw_url = file.get('raw_url') \n",
    "                \n",
    "                # All the file metrics here\n",
    "                all_file_details.append({\n",
    "                    \"filename\": filename,\n",
    "                    #\"patch\": final_patch,\n",
    "                    \"status\": file.get('status'),\n",
    "                    #\"additions\": file.get('additions', 0),\n",
    "                    #\"deletions\": file.get('deletions', 0),\n",
    "                    \"raw_url\": raw_url \n",
    "                })\n",
    "            \n",
    "            # Check for the next page header\n",
    "            if 'link' not in response.headers or 'rel=\"next\"' not in response.headers['link']:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during API call on page {page}: {e}\")\n",
    "            break\n",
    "            \n",
    "    print(f\"Finished fetching. Total files processed: {len(all_file_details)}\")\n",
    "    return all_file_details\n",
    "        \n",
    "# ============================================================\n",
    "# Main Helper: Fetch the main metric functions \n",
    "# ============================================================\n",
    "def fetch_metrics(repo_list, token):\n",
    "    results = []\n",
    "    # limit the number of repositories processed here for testing REPOSITORIES[:10]:\n",
    "    for owner, repo, pr_number in repo_list[:2]: # Apply the test limit here\n",
    "        metrics = get_pr_file_details(owner, repo, pr_number, token)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    \n",
    "    # Create the Metric DataFrame\n",
    "    return results # Return a List[List[Dict]] to be process later\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "files_list_accepted = fetch_metrics(ACCEPTED_PULL_REQUEST, GITHUB_TOKEN)\n",
    "files_list_rejected = fetch_metrics(REJECTED_PULL_REQUEST, GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab384eb",
   "metadata": {},
   "source": [
    "## 3.2. Restructure the File List and Filter the Java files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting filtering and sorting\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Filter and Aggregate PR Data\n",
    "# ============================================================\n",
    "def filter_and_aggregate_pr_data(pr_files_list: List[List[Dict]], repo_list: List[Tuple[str, str, int]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filters file details for Java files that are not deleted and aggregates the \n",
    "    relevant data (like raw_urls) at the Pull Request level.\n",
    "\n",
    "    Args:\n",
    "        pr_files_list: The nested list of file details from the GitHub API.\n",
    "        repo_list: The original list of (owner, repo, pr_number) tuples \n",
    "                   used to fetch the data.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, one for each PR, containing aggregated metrics.\n",
    "    \"\"\"\n",
    "    aggregated_pr_data = []\n",
    "    \n",
    "    # Iterate through the results for each PR\n",
    "    for pr_index, pr_files in enumerate(pr_files_list):\n",
    "        \n",
    "        # Safely retrieve metadata for the current PR\n",
    "        if pr_index >= len(repo_list):\n",
    "            print(f\"Warning: Missing metadata for PR at index {pr_index}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        owner, repo, pr_number = repo_list[pr_index]\n",
    "        \n",
    "        java_files_to_analyze = []\n",
    "        \n",
    "        # --- File-level filtering ---\n",
    "        for file in pr_files:\n",
    "            filename = file.get('filename', '')\n",
    "            status = file.get('status', '')\n",
    "            \n",
    "            # 1. Detect .java file in the file name (case-insensitive)\n",
    "            is_java = filename.lower().endswith('.java')\n",
    "            \n",
    "            # 2. Exclude status deleted (we only analyze added or modified code)\n",
    "            is_not_deleted = status != 'deleted'\n",
    "            \n",
    "            # Store the file name and raw URL for the non-deleted Java file\n",
    "            if is_java and is_not_deleted:\n",
    "                java_files_to_analyze.append({\n",
    "                    \"file_name\": filename,\n",
    "                    \"raw_url\": file.get('raw_url')\n",
    "                })\n",
    "        \n",
    "        # --- PR-level aggregation ---\n",
    "        aggregated_pr_data.append({\n",
    "            'owner': owner,\n",
    "            'repo': repo,\n",
    "            'pr_number': pr_number,\n",
    "            'java_files_analyzed_count': len(java_files_to_analyze),\n",
    "            'files_to_analyze': java_files_to_analyze, # List[Dict]\n",
    "            # add the PMD violation counts later\n",
    "            'pmd_violations': {} \n",
    "        })\n",
    "        \n",
    "    return aggregated_pr_data\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM: \n",
    "# ============================================================\n",
    "print(\"\\nStarting filtering and sorting\")\n",
    "pr_code_metrics_filtered_accepted = filter_and_aggregate_pr_data(files_list_accepted, ACCEPTED_PULL_REQUEST)\n",
    "pr_code_metrics_filtered_rejected = filter_and_aggregate_pr_data(files_list_accepted, ACCEPTED_PULL_REQUEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66502b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'owner': 'dotCMS', 'repo': 'core', 'pr_number': 32609, 'java_files_analyzed_count': 0, 'files_to_analyze': [], 'pmd_violations': {}}, {'owner': 'apache', 'repo': 'pulsar', 'pr_number': 24542, 'java_files_analyzed_count': 4, 'files_to_analyze': [{'file_name': 'microbench/src/main/java/org/apache/pulsar/broker/delayed/bucket/BucketDelayedDeliveryTrackerSimpleBenchmark.java', 'raw_url': 'https://github.com/apache/pulsar/raw/975ff1f23d579ffc49fbc12a86def464254de4d9/microbench%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fpulsar%2Fbroker%2Fdelayed%2Fbucket%2FBucketDelayedDeliveryTrackerSimpleBenchmark.java'}, {'file_name': 'microbench/src/main/java/org/apache/pulsar/broker/delayed/bucket/package-info.java', 'raw_url': 'https://github.com/apache/pulsar/raw/975ff1f23d579ffc49fbc12a86def464254de4d9/microbench%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fpulsar%2Fbroker%2Fdelayed%2Fbucket%2Fpackage-info.java'}, {'file_name': 'pulsar-broker/src/main/java/org/apache/pulsar/broker/delayed/bucket/BucketDelayedDeliveryTracker.java', 'raw_url': 'https://github.com/apache/pulsar/raw/975ff1f23d579ffc49fbc12a86def464254de4d9/pulsar-broker%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fpulsar%2Fbroker%2Fdelayed%2Fbucket%2FBucketDelayedDeliveryTracker.java'}, {'file_name': 'pulsar-broker/src/test/java/org/apache/pulsar/broker/delayed/bucket/BucketDelayedDeliveryTrackerThreadSafetyTest.java', 'raw_url': 'https://github.com/apache/pulsar/raw/975ff1f23d579ffc49fbc12a86def464254de4d9/pulsar-broker%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fpulsar%2Fbroker%2Fdelayed%2Fbucket%2FBucketDelayedDeliveryTrackerThreadSafetyTest.java'}], 'pmd_violations': {}, 'pmd_report_path': './pr_analysis_staging\\\\apache\\\\pulsar\\\\24542\\\\pmd_report_24542.xml'}]\n"
     ]
    }
   ],
   "source": [
    "print(pr_code_metrics_filtered_accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803e0e8",
   "metadata": {},
   "source": [
    "## 3.3. Download the changed Java files and do PMD static code analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afbc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing PMD for PR #32609: pmd.bat check -d ./pr_analysis_staging\\dotCMS\\core\\32609 -R D:/Program Files/PMD/pmd-bin-7.18.0/rulesets/java/quickstart.xml -f xml -r ./pr_analysis_staging\\dotCMS\\core\\32609\\pmd_report_32609.xml\n",
      "PMD command failed (code 1). Error:\n",
      "[ERROR] No such file .\\pr_analysis_staging\\dotCMS\\core\\32609\n",
      "[ERROR] Could not initialize analysis: java.nio.file.NoSuchFileException: .\\pr_analysis_staging\\dotCMS\\core\\32609\\pmd_report_32609.xml\n",
      "\n",
      "Skipping filtering for PR #32609 due to PMD failure.\n",
      "Executing PMD for PR #24542: pmd.bat check -d ./pr_analysis_staging\\apache\\pulsar\\24542 -R D:/Program Files/PMD/pmd-bin-7.18.0/rulesets/java/quickstart.xml -f xml -r ./pr_analysis_staging\\apache\\pulsar\\24542\\pmd_report_24542.xml\n",
      "PMD analysis completed. Report saved to: ./pr_analysis_staging\\apache\\pulsar\\24542\\pmd_report_24542.xml\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: \n",
    "# ============================================================\n",
    "def download_file(raw_url, local_path, token=None):\n",
    "    \"\"\"Downloads a single file from GitHub's raw URL.\"\"\"\n",
    "    headers = {}\n",
    "    if token:\n",
    "        headers['Authorization'] = f\"token {token}\"\n",
    "        \n",
    "    response = requests.get(raw_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        with open(local_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download {raw_url}. Status: {response.status_code}\")\n",
    "        return False\n",
    "    \n",
    "# ============================================================\n",
    "# Helper: \n",
    "# ============================================================\n",
    "PMD_EXECUTABLE = \"pmd.bat\"\n",
    "RULESET_PATH = \"D:/Program Files/PMD/pmd-bin-7.18.0/rulesets/java/quickstart.xml\"\n",
    "BASE_STAGING_DIR = \"./pr_analysis_staging\"\n",
    "\n",
    "def run_pmd_for_pr(owner: str, repo: str, pr_number: int, ruleset_path: str = RULESET_PATH) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Runs PMD on the staging directory for a specific Pull Request.\n",
    "\n",
    "    Args:\n",
    "        owner: The repository owner.\n",
    "        repo: The repository name.\n",
    "        pr_number: The Pull Request number.\n",
    "        ruleset_path: Path to the PMD ruleset XML file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated XML report file, or None if the analysis failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dynamically define the source directory based on the download structure\n",
    "    source_dir = os.path.join(BASE_STAGING_DIR, owner, repo, str(pr_number))\n",
    "    \n",
    "    # Define a unique output file path inside the PR's staging folder\n",
    "    output_file_name = f\"pmd_report_{pr_number}.xml\"\n",
    "    output_file_path = os.path.join(source_dir, output_file_name)\n",
    "\n",
    "    # 1. Construct the PMD command\n",
    "    pmd_command = [\n",
    "        PMD_EXECUTABLE,\n",
    "        \"check\",\n",
    "        \"-d\", source_dir,\n",
    "        \"-R\", ruleset_path,\n",
    "        \"-f\", \"xml\",\n",
    "        \"-r\", output_file_path\n",
    "    ]\n",
    "    \n",
    "    print(f\"Executing PMD for PR #{pr_number}: {' '.join(pmd_command)}\")\n",
    "\n",
    "    try:\n",
    "        # 2. Execute the command\n",
    "        result = subprocess.run(\n",
    "            pmd_command, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            check=False # PMD returns exit code 4 on violations\n",
    "        )\n",
    "        \n",
    "        # Check success (0 or 4)\n",
    "        if result.returncode == 0 or result.returncode == 4:\n",
    "            print(f\"PMD analysis completed. Report saved to: {output_file_path}\")\n",
    "            return output_file_path\n",
    "        else:\n",
    "            print(f\"PMD command failed (code {result.returncode}). Error:\\n{result.stderr}\")\n",
    "            return None\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PMD executable '{PMD_EXECUTABLE}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during PMD execution: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# Main: \n",
    "# ============================================================\n",
    "for pr_data in pr_code_metrics_filtered_accepted:\n",
    "    owner = pr_data['owner']\n",
    "    repo = pr_data['repo']\n",
    "    pr_number = pr_data['pr_number']\n",
    "    files_to_analyze = pr_data['files_to_analyze'] \n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Checkout the PR branch and download the right changed files\n",
    "    # ------------------------------------------------------------------\n",
    "    # Create a local staging directory structure. Prevents file path clashes and organizes your PMD reports\n",
    "    base_dir = \"./pr_analysis_staging\"\n",
    "    local_staging_dir = os.path.join(base_dir, owner, repo, str(pr_number))\n",
    "    \n",
    "    print(f\"\\n--- Processing PR: {owner}/{repo} #{pr_number} ({len(files_to_analyze)} files) ---\")\n",
    "    \n",
    "    # Inner loop: Iterate over the list of file dictionaries\n",
    "    for file_data in files_to_analyze:\n",
    "        # Get the two keys you need from the file dictionary\n",
    "        file_name = file_data['file_name']\n",
    "        raw_url = file_data['raw_url']\n",
    "        \n",
    "        # Determine the full local path for this file\n",
    "        local_path = os.path.join(local_staging_dir, file_name)\n",
    "        \n",
    "        download_file(raw_url, local_path, GITHUB_TOKEN)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Run PMD Analysis on the downloaded files\n",
    "    # ------------------------------------------------------------------\n",
    "    pmd_report_path = run_pmd_for_pr(owner, repo, pr_number)\n",
    "    \n",
    "    if pmd_report_path:\n",
    "        # Store the report path in your PR data dictionary for later parsing\n",
    "        pr_data['pmd_report_path'] = pmd_report_path\n",
    "        \n",
    "        # 3. Next step: Parse the XML report and filter by the patch/diff data\n",
    "        # (This will be the next piece of logic you need to write!)\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Skipping filtering for PR #{pr_number} due to PMD failure.\")\n",
    "\n",
    "# Now, pr_code_metrics_filtered_accepted contains the path to the PMD report for each PR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9f00e",
   "metadata": {},
   "source": [
    "# 4. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pr_metrics_df_accept' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# MAIN PROGRAM - Separate Processing\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# --- Processing Accepted Repositories ---\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing Accepted Repositories ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m final_df_accept \u001b[38;5;241m=\u001b[39m finalize_dataframe(\n\u001b[1;32m---> 42\u001b[0m     pr_metrics_df_accept, \n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpr_metrics_accepted.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Save to a separate file\u001b[39;00m\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAccepted Repository Metrics DataFrame Created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows in Accepted DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_df_accept)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pr_metrics_df_accept' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'PR_ID': 'PR_number',\n",
    "        'NumCommits': 'Commits', \n",
    "        'NumComments': 'Comments', \n",
    "        'NumFormalReviews': 'Formal_Review', \n",
    "        'NumInlineComments': 'Inline_Comments_Review'\n",
    "    }\n",
    "    final_df = metrics_df.rename(columns=rename_map)\n",
    "\n",
    "    # 3. Define the final column order\n",
    "    column_order = [\n",
    "        'Repo', 'PR_number', 'Commits', 'Additions', 'Deletions', 'Files_Changed', 'Comments', 'Formal_Review', \n",
    "        'Inline_Comments_Review', 'NumPathsInFile', 'AvgPathCharLength', 'MaxPathCharLength', \n",
    "    ]\n",
    "    \n",
    "    # Apply the final column order\n",
    "    final_df = final_df[column_order]\n",
    "    final_df = final_df.fillna(0.0)\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    pr_metrics_df_accept, \n",
    "    \"pr_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    pr_metrics_df_reject, \n",
    "    \"pr_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
