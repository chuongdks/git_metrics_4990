{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import syntok.segmenter as segmenter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313c661",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb8df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Features Dimension: Repository\n",
    "repo_feat_accept_df = pd.read_csv('./1_Repository DImension/repo_metrics_accepted_2.csv')\n",
    "repo_feat_reject_df = pd.read_csv('./1_Repository DImension/repo_metrics_rejected_2.csv')\n",
    "\n",
    "# 2nd Features Dimension: Pull Request\n",
    "pr_feat_accept_df = pd.read_csv('./2_Pull Request Dimension/pr_metrics_accepted.csv')\n",
    "pr_feat_reject_df = pd.read_csv('./2_Pull Request Dimension/pr_metrics_rejected.csv')\n",
    "\n",
    "# 3rd Features Dimension: Developer Experience\n",
    "pr_dev_accept_df = pd.read_csv('./3_Dev_Experience/pr_dev_metrics_accepted.csv')\n",
    "pr_dev_reject_df = pd.read_csv('./3_Dev_Experience/pr_dev_metrics_rejected.csv')\n",
    "\n",
    "# 5th Features Dimension: Readability\n",
    "pr_read_accept_df = pd.read_csv('./5_Readability/issue_pr_readability_accepted.csv')\n",
    "pr_read_reject_df = pd.read_csv('./5_Readability/issue_pr_readability_rejected.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61692e",
   "metadata": {},
   "source": [
    "# Combined and Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf17b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Combined all the dataframes \n",
    "# ============================================================\n",
    "# --- 1st Features Dimension: Repository ---\n",
    "columns_to_drop = ['Repo']\n",
    "rename_map_1 = {'PR_number': 'PR_Number'}\n",
    "pr_accept_df = pr_feat_accept_df.drop(columns=columns_to_drop, axis=1, inplace=False).rename(columns=rename_map_1)\n",
    "pr_reject_df = pr_feat_reject_df.drop(columns=columns_to_drop, axis=1, inplace=False).rename(columns=rename_map_1)\n",
    "pr_accept_df['Result'] = 1\n",
    "pr_reject_df['Result'] = 0\n",
    "combined_df = pd.concat([pr_accept_df, pr_reject_df], ignore_index=True)\n",
    "\n",
    "# --- 2nd Features Dimension: Pull Request ---\n",
    "columns_to_drop_2 = ['Repo', 'NumPathsInFile', 'AvgPathCharLength', 'MaxPathCharLength']\n",
    "rename_map_2 = {'PR_number': 'PR_Number'}\n",
    "pr_accept_df_2 = pr_feat_accept_df.drop(columns=columns_to_drop_2, axis=1, inplace=False).rename(columns=rename_map_2)\n",
    "pr_reject_df_2 = pr_feat_reject_df.drop(columns=columns_to_drop_2, axis=1, inplace=False).rename(columns=rename_map_2)\n",
    "pr_accept_df_2['Result'] = 1\n",
    "pr_reject_df_2['Result'] = 0\n",
    "combined_df_2 = pd.concat([pr_accept_df_2, pr_reject_df_2], ignore_index=True)\n",
    "\n",
    "# --- 3rd Features Dimension: Developer Experience ---\n",
    "columns_to_drop_3 = ['Repo', 'Creation_Date', 'User', 'Total_Commits_Repo', 'Creation_Date']\n",
    "pr_accept_df_3 = pr_dev_accept_df.drop(columns=columns_to_drop_3, axis=1, inplace=False)\n",
    "pr_reject_df_3 = pr_dev_reject_df.drop(columns=columns_to_drop_3, axis=1, inplace=False)\n",
    "pr_accept_df_3['Result'] = 1\n",
    "pr_reject_df_3['Result'] = 0\n",
    "combined_df_3 = pd.concat([pr_accept_df_3, pr_reject_df_3], ignore_index=True)\n",
    "\n",
    "# --- 5th Features Dimension: Readability ---\n",
    "columns_to_drop_5 = ['type', 'pr_title', 'pr_body', 'issue_number', 'issue_title', 'issue_body']\n",
    "rename_map_5 = {'pr_number': 'PR_Number'}\n",
    "pr_accept_df_5 = pr_read_accept_df.drop(columns=columns_to_drop_5, axis=1, inplace=False).rename(columns=rename_map_5)\n",
    "pr_reject_df_5 = pr_read_reject_df.drop(columns=columns_to_drop_5, axis=1, inplace=False).rename(columns=rename_map_5)\n",
    "pr_accept_df_5['Result'] = 1\n",
    "pr_reject_df_5['Result'] = 0\n",
    "combined_df_5 = pd.concat([pr_accept_df_5, pr_reject_df_5], ignore_index=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Perform Left Joins\n",
    "# ============================================================\n",
    "\n",
    "# Drop redundant 'Result' column from subsequent merges\n",
    "combined_df_2_cleaned = combined_df_2.drop(columns=['Result'])\n",
    "combined_df_3_cleaned = combined_df_3.drop(columns=['Result'])\n",
    "combined_df_5_cleaned = combined_df_5.drop(columns=['Result'])\n",
    "\n",
    "final_combined_df = combined_df.copy()\n",
    "\n",
    "# Left Merge: Keeps all rows from combined_df_2, adding features from combined_df_3\n",
    "final_combined_df = pd.merge(\n",
    "    final_combined_df,\n",
    "    combined_df_2_cleaned,\n",
    "    on='PR_Number',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Left Merge: Keeps all rows from the previous step, adding features from combined_df_3\n",
    "final_combined_df = pd.merge(\n",
    "    final_combined_df,\n",
    "    combined_df_3_cleaned,\n",
    "    on='PR_Number',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Left Merge: Keeps all rows from the previous step, adding features from combined_df_5\n",
    "final_combined_df = pd.merge(\n",
    "    final_combined_df,\n",
    "    combined_df_5_cleaned,\n",
    "    on='PR_Number',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Fill NaNs with 0\n",
    "# ============================================================\n",
    "final_combined_df.fillna(0, inplace=True)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Finalize and Display\n",
    "# ============================================================\n",
    "# Set PR_Number as the index\n",
    "final_combined_df.set_index('PR_Number', inplace=True)\n",
    "final_combined_df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# Reorder columns to put Result last\n",
    "cols = final_combined_df.columns.tolist()\n",
    "result_col = 'Result'\n",
    "feature_cols_final = [col for col in cols if col != result_col]\n",
    "new_col_order = feature_cols_final + [result_col]\n",
    "final_combined_df = final_combined_df[new_col_order]\n",
    "\n",
    "final_combined_df.to_csv(\"./test_ML_datasets.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed34f3e",
   "metadata": {},
   "source": [
    "# Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f3b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': {'Accuracy': 0.8658346333853354, 'Classification Report': '              precision    recall  f1-score   support\\n\\nRejected (0)       0.74      0.28      0.40       105\\nAccepted (1)       0.87      0.98      0.92       536\\n\\n    accuracy                           0.87       641\\n   macro avg       0.81      0.63      0.66       641\\nweighted avg       0.85      0.87      0.84       641\\n'}, 'Decision Tree': {'Accuracy': 0.8705148205928237, 'Classification Report': '              precision    recall  f1-score   support\\n\\nRejected (0)       0.60      0.61      0.61       105\\nAccepted (1)       0.92      0.92      0.92       536\\n\\n    accuracy                           0.87       641\\n   macro avg       0.76      0.77      0.76       641\\nweighted avg       0.87      0.87      0.87       641\\n'}, 'Random Forest': {'Accuracy': 0.9157566302652106, 'Classification Report': '              precision    recall  f1-score   support\\n\\nRejected (0)       0.78      0.68      0.72       105\\nAccepted (1)       0.94      0.96      0.95       536\\n\\n    accuracy                           0.92       641\\n   macro avg       0.86      0.82      0.84       641\\nweighted avg       0.91      0.92      0.91       641\\n'}, 'XGBoost': {'Accuracy': 0.9141965678627145, 'Classification Report': '              precision    recall  f1-score   support\\n\\nRejected (0)       0.76      0.70      0.73       105\\nAccepted (1)       0.94      0.96      0.95       536\\n\\n    accuracy                           0.91       641\\n   macro avg       0.85      0.83      0.84       641\\nweighted avg       0.91      0.91      0.91       641\\n'}}\n",
      "\n",
      "--- DETAILED CLASSIFICATION REPORTS ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Rejected (0)       0.74      0.28      0.40       105\n",
      "Accepted (1)       0.87      0.98      0.92       536\n",
      "\n",
      "    accuracy                           0.87       641\n",
      "   macro avg       0.81      0.63      0.66       641\n",
      "weighted avg       0.85      0.87      0.84       641\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Rejected (0)       0.60      0.61      0.61       105\n",
      "Accepted (1)       0.92      0.92      0.92       536\n",
      "\n",
      "    accuracy                           0.87       641\n",
      "   macro avg       0.76      0.77      0.76       641\n",
      "weighted avg       0.87      0.87      0.87       641\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Rejected (0)       0.78      0.68      0.72       105\n",
      "Accepted (1)       0.94      0.96      0.95       536\n",
      "\n",
      "    accuracy                           0.92       641\n",
      "   macro avg       0.86      0.82      0.84       641\n",
      "weighted avg       0.91      0.92      0.91       641\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Rejected (0)       0.76      0.70      0.73       105\n",
      "Accepted (1)       0.94      0.96      0.95       536\n",
      "\n",
      "    accuracy                           0.91       641\n",
      "   macro avg       0.85      0.83      0.84       641\n",
      "weighted avg       0.91      0.91      0.91       641\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:25:05] WARNING: C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Separate Features (X) and Target (y)\n",
    "# PR_Number is already the index and is excluded from features X\n",
    "X = final_combined_df.drop(columns=['Result'])\n",
    "y = final_combined_df['Result']\n",
    "\n",
    "# 2. Split the data into training (70%) and testing (30%) sets\n",
    "# Using stratify=y ensures the proportion of Accepted/Rejected PRs is maintained in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Define the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# 5. Train, Predict, and Evaluate\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data only for Logistic Regression\n",
    "    if name == \"Logistic Regression\":\n",
    "        model.fit(X_train_scaled, y_train)      # train model\n",
    "        y_pred = model.predict(X_test_scaled)   # Make prediction\n",
    "    else:\n",
    "        # Tree-based models (DT, RF, XGB) do not require scaling\n",
    "        model.fit(X_train, y_train)             # train model\n",
    "        y_pred = model.predict(X_test)          # Make prediction\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Classification Report': classification_report(y_test, y_pred, target_names=['Rejected (0)', 'Accepted (1)'])\n",
    "    }\n",
    "    \n",
    "print(results)   \n",
    "\n",
    "# 6. Print Summary of Results\n",
    "print(\"\\n--- DETAILED CLASSIFICATION REPORTS ---\")\n",
    "for name, res in results.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(res['Classification Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f75d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative score in issue body (Accepted PRs): 12\n",
      "Number of negative score in issue body (Rejected PRs): 17\n",
      "Number of negative score in PR body (Accepted PRs): 7\n",
      "Number of negative score in PR body (Rejected PRs): 9\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths as provided in the prompt\n",
    "file_accept_path = './5_Readability/issue_pr_readability_accepted.csv'\n",
    "file_reject_path = './5_Readability/issue_pr_readability_rejected.csv'\n",
    "\n",
    "# Read the dataframes\n",
    "pr_read_accept_df = pd.read_csv(file_accept_path)\n",
    "pr_read_reject_df = pd.read_csv(file_reject_path)\n",
    "\n",
    "# Count the number of negative decimal numbers in the 'issue_readability' column\n",
    "# A negative decimal number will be < 0\n",
    "negative_accept_count = (pr_read_accept_df['issue_readability'] < 0).sum()\n",
    "negative_reject_count = (pr_read_reject_df['issue_readability'] < 0).sum()\n",
    "\n",
    "negative_accept_count_pr = (pr_read_accept_df['pr_readability'] < 0).sum()\n",
    "negative_reject_count_pr = (pr_read_reject_df['pr_readability'] < 0).sum()\n",
    "\n",
    "print(f\"Number of negative score in issue body (Accepted PRs): {negative_accept_count}\")\n",
    "print(f\"Number of negative score in issue body (Rejected PRs): {negative_reject_count}\")\n",
    "\n",
    "print(f\"Number of negative score in PR body (Accepted PRs): {negative_accept_count_pr}\")\n",
    "print(f\"Number of negative score in PR body (Rejected PRs): {negative_reject_count_pr}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
