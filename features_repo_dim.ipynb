{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05532dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce3c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./api_key.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b02dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Pull Request\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\", columns=['repo_id', 'merged_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a36c04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2807 repos down to 86 using language='Java'.\n"
     ]
    }
   ],
   "source": [
    "# Filter for Repo that has Java language\n",
    "initial_count = len(repo_df)\n",
    "repo_df = repo_df[repo_df['language'] == 'Java']\n",
    "print(f\"Filtered {initial_count} repos down to {len(repo_df)} using language='Java'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63114452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for 86 unique repositories ready for merge.\n"
     ]
    }
   ],
   "source": [
    "# Join Repo and PR table based on repo id\n",
    "merged_pr_df = pr_df.merge(\n",
    "        repo_df[['id', 'full_name']], \n",
    "        left_on='repo_id', \n",
    "        right_on='id', \n",
    "        how='inner' \n",
    "    ).drop(columns=['id', 'repo_id'])\n",
    "\n",
    "# Filter PRs that were rejected (not merged)\n",
    "merged_pr_df['is_accepted'] = merged_pr_df['merged_at'].notnull()\n",
    "\n",
    "# Group and Aggregate: Group by the new 'full_name' column\n",
    "pr_metrics = merged_pr_df.groupby('full_name').agg(\n",
    "    total_prs=('is_accepted', 'size'),        \n",
    "    accepted_prs=('is_accepted', 'sum')       \n",
    ").reset_index()\n",
    "\n",
    "# Calculate Rejected: Subtract accepted from total\n",
    "pr_metrics['rejected_prs'] = pr_metrics['total_prs'] - pr_metrics['accepted_prs']\n",
    "\n",
    "# Prepare for Merge: Rename the key column\n",
    "pr_metrics = pr_metrics[['full_name', 'accepted_prs', 'rejected_prs']]\n",
    "\n",
    "# print to csv for checking\n",
    "print(f\"Aggregated metrics for {len(pr_metrics)} unique repositories ready for merge.\")\n",
    "pr_metrics.to_csv(\"accepted_rejected_repo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c0c5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepted PR if accepted_pr >= rejected_pr,\n",
    "pr_metrics['pr_status'] = np.where(\n",
    "    pr_metrics['accepted_prs'] >= pr_metrics['rejected_prs'], \n",
    "    'Accepted', \n",
    "    'Rejected'\n",
    ")\n",
    "\n",
    "# Filter the DataFrame into two parts\n",
    "accepted_repos_df = pr_metrics[pr_metrics['pr_status'] == 'Accepted']\n",
    "rejected_repos_df = pr_metrics[pr_metrics['pr_status'] == 'Rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edfa6d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 entries of the ACCEPTED REPOSITORIES list:\n",
      "[('Camelcade', 'Perl5-IDEA'), ('DataDog', 'dd-trace-java'), ('EduMIPS64', 'edumips64'), ('JetBrains', 'psiviewer'), ('OWASP', 'wrongsecrets')]\n",
      "\n",
      "First 5 entries of the REJECTED REPOSITORIES list:\n",
      "[('1c-syntax', 'bsl-language-server'), ('2006-Scape', '2006Scape'), ('AutoMQ', 'automq'), ('Azure-Samples', 'azure-search-openai-demo-java'), ('Azure', 'azure-sdk-for-java')]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Split the name and put it in a List of Dict (not needed but ehh accidentally made the method like that)\n",
    "# ============================================================\n",
    "def process_repositories(pr_metrics_df, status_filter):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame by status, splits the full_name, and creates a \n",
    "    list of (owner, repo) tuples for processing.\n",
    "    \"\"\"\n",
    "    # 1. Filter the DataFrame by status\n",
    "    repos_df = pr_metrics_df[pr_metrics_df['pr_status'] == status_filter]\n",
    "    \n",
    "    # 2. Split the name into (owner, repo) tuples\n",
    "    # Use .str.split().apply(tuple) to get a list of tuples directly (more Pythonic)\n",
    "    repositories = repos_df['full_name'].str.split('/', n=1, expand=True).apply(tuple, axis=1).tolist()\n",
    "    \n",
    "    print(f\"\\nFirst 5 entries of the {status_filter.upper()} REPOSITORIES list:\")\n",
    "    print(repositories[:5])\n",
    "    \n",
    "    return repositories\n",
    "\n",
    "\n",
    "ACCEPTED_REPOSITORIES = process_repositories(pr_metrics, 'Accepted')\n",
    "REJECTED_REPOSITORIES = process_repositories(pr_metrics, 'Rejected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "451c76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data retrieval... (may take a moment due to multiple API calls)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Total Contributors\n",
    "# ============================================================\n",
    "def get_total_contributors(owner, repo, headers):\n",
    "    \"\"\"\n",
    "    Retrieves the total number of contributors for a GitHub repository \n",
    "    using the Link header pagination trick.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n",
    "    params = {\"per_page\": 1, \"anon\": \"true\"}  # include anonymous contributors\n",
    "\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        link_header = response.headers.get('Link')\n",
    "\n",
    "        if link_header:\n",
    "            match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        # fallback for small repos\n",
    "        return 1\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching contributors for {owner}/{repo}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper: Commit Count\n",
    "# ============================================================\n",
    "def get_commit_count(owner, repo, headers):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "    params = {\"per_page\": 1}\n",
    "\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        link_header = response.headers.get('Link')\n",
    "\n",
    "        if link_header:\n",
    "            match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "\n",
    "        # fallback for repos with few commits\n",
    "        response = requests.get(url, headers=headers, params={\"per_page\": 100}, timeout=10)\n",
    "        return len(response.json())\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching commits for {owner}/{repo}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ============================================================\n",
    "# Helper: Issues and Pull Requests\n",
    "# ============================================================\n",
    "def get_issue_and_pull_metrics(owner, repo, headers):\n",
    "    base_search_url = \"https://api.github.com/search/issues\"\n",
    "\n",
    "    queries = {\n",
    "        'open_issues': f\"repo:{owner}/{repo} is:issue is:open\",\n",
    "        'closed_issues': f\"repo:{owner}/{repo} is:issue is:closed\",\n",
    "        'open_prs': f\"repo:{owner}/{repo} is:pr is:open\",\n",
    "        'closed_prs': f\"repo:{owner}/{repo} is:pr is:closed\",\n",
    "    }\n",
    "\n",
    "    metrics = {'open_issues': 0, 'closed_issues': 0, 'total_pull_requests': 0}\n",
    "\n",
    "    for key, query in queries.items():\n",
    "        try:\n",
    "            response = requests.get(base_search_url, headers=headers, params={'q': query, 'per_page': 1}, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            total = response.json().get('total_count', 0)\n",
    "\n",
    "            if key.startswith('open_issue'):\n",
    "                metrics['open_issues'] = total\n",
    "            elif key.startswith('closed_issue'):\n",
    "                metrics['closed_issues'] = total\n",
    "            else:\n",
    "                metrics['total_pull_requests'] += total\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ============================================================\n",
    "# Main Function: Repository Metrics\n",
    "# ============================================================\n",
    "def get_repo_metrics(owner, repo, github_token=None):\n",
    "    \"\"\"\n",
    "    Retrieves metrics for a GitHub repository:\n",
    "    Watchers, Duration, Commits, Issues, Pull Requests, Contributors\n",
    "    \"\"\"\n",
    "    repo_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "    headers = {}\n",
    "    if github_token:\n",
    "        headers[\"Authorization\"] = f\"token {github_token}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(repo_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        repo_data = response.json()\n",
    "\n",
    "        # Basic metrics\n",
    "        num_watchers = repo_data.get('stargazers_count', 0)\n",
    "        num_commits = get_commit_count(owner, repo, headers)\n",
    "        issue_pr_metrics = get_issue_and_pull_metrics(owner, repo, headers)\n",
    "        num_closed_issues = issue_pr_metrics['closed_issues']\n",
    "        num_open_issues = issue_pr_metrics['open_issues']\n",
    "        num_pull_reqs = issue_pr_metrics['total_pull_requests']\n",
    "        num_contributors = get_total_contributors(owner, repo, headers)\n",
    "\n",
    "        # Duration (days since creation)\n",
    "        created_at_str = repo_data.get('created_at')\n",
    "        num_days = 0\n",
    "        if created_at_str:\n",
    "            created_at_aware = parser.parse(created_at_str)\n",
    "            now_utc = datetime.now(timezone.utc)\n",
    "            num_days = (now_utc - created_at_aware).days\n",
    "\n",
    "        return {\n",
    "            \"Repo\": f\"{owner}/{repo}\",\n",
    "            \"NumWatchers\": num_watchers,\n",
    "            \"NumDays\": num_days,\n",
    "            \"NumCommits\": num_commits,\n",
    "            \"NumOpenIssues\": num_open_issues,\n",
    "            \"NumClosedIssues\": num_closed_issues,\n",
    "            \"NumPullReqs\": num_pull_reqs,\n",
    "            \"NumContributors\": num_contributors,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {owner}/{repo}: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ============================================================\n",
    "# Helper: Fetch the main metric functions \n",
    "# ============================================================\n",
    "def fetch_metrics(repo_list, token):\n",
    "    results = []\n",
    "    # limit the number of repositories processed here for testing REPOSITORIES[:10]:\n",
    "    for owner, repo in repo_list[:10]: # Apply the test limit here\n",
    "        metrics = get_repo_metrics(owner, repo, token)\n",
    "        if metrics:\n",
    "            results.append(metrics)\n",
    "    \n",
    "    # Create the Metric DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM\n",
    "# ============================================================\n",
    "print(\"\\nStarting data retrieval... (may take a moment due to multiple API calls)\")\n",
    "metrics_df_accept = fetch_metrics(ACCEPTED_REPOSITORIES, GITHUB_TOKEN)\n",
    "metrics_df_reject = fetch_metrics(REJECTED_REPOSITORIES, GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3d20c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Accepted Repositories ---\n",
      "\n",
      "Accepted Repository Metrics DataFrame Created:\n",
      "Total rows in Accepted DataFrame: 10\n",
      "\n",
      "--- Processing Rejected Repositories ---\n",
      "\n",
      "Rejected Repository Metrics DataFrame Created:\n",
      "Total rows in Rejected DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Helper: Finalize the dataframe, adding stars and forks\n",
    "# ============================================================\n",
    "def finalize_dataframe(metrics_df, repo_df, output_filename):\n",
    "    \"\"\"\n",
    "    Applies the merging, cleaning, renaming, and reordering steps \n",
    "    to a single metrics DataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Merge with the original repo_df to get stars and forks\n",
    "    final_df = pd.merge(\n",
    "        metrics_df, \n",
    "        repo_df[['full_name', 'stars', 'forks']],\n",
    "        left_on='Repo', \n",
    "        right_on='full_name', \n",
    "        how='left' \n",
    "    )\n",
    "\n",
    "    # 2. Clean up and reorder columns\n",
    "    final_df = final_df.drop(columns=['full_name'])\n",
    "    \n",
    "    # Define the rename mapping\n",
    "    rename_map = {\n",
    "        'NumWatchers': 'Watchers',\n",
    "        'NumDays': 'Duration_Days',\n",
    "        'NumCommits': 'Commits', \n",
    "        'NumOpenIssues': 'Open_Issues', \n",
    "        'NumClosedIssues': 'Closed_Issues', \n",
    "        'NumPullReqs': 'Pull_Requests', \n",
    "        'NumContributors': 'Contributors',\n",
    "        'stars': 'Stars',\n",
    "        'forks': 'Forks' \n",
    "    }\n",
    "    final_df = final_df.rename(columns=rename_map)\n",
    "\n",
    "    # 3. Define the final column order\n",
    "    column_order = [\n",
    "        'Repo', 'Contributors', 'Stars', 'Forks', 'Watchers', 'Duration_Days', \n",
    "        'Commits', 'Open_Issues', 'Closed_Issues', 'Pull_Requests', \n",
    "    ]\n",
    "    \n",
    "    # Apply the final column order\n",
    "    final_df = final_df[column_order]\n",
    "\n",
    "    # 4. Save the file (using CSV as per your original request)\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PROGRAM - Separate Processing\n",
    "# ============================================================\n",
    "\n",
    "# --- Processing Accepted Repositories ---\n",
    "print(\"\\n--- Processing Accepted Repositories ---\")\n",
    "final_df_accept = finalize_dataframe(\n",
    "    metrics_df_accept, \n",
    "    repo_df, \n",
    "    \"repo_metrics_accepted.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nAccepted Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Accepted DataFrame: {len(final_df_accept)}\")\n",
    "\n",
    "# --- Processing Rejected Repositories ---\n",
    "print(\"\\n--- Processing Rejected Repositories ---\")\n",
    "final_df_reject = finalize_dataframe(\n",
    "    metrics_df_reject, \n",
    "    repo_df, \n",
    "    \"repo_metrics_rejected.csv\" # Save to a separate file\n",
    ")\n",
    "\n",
    "print(\"\\nRejected Repository Metrics DataFrame Created:\")\n",
    "print(f\"Total rows in Rejected DataFrame: {len(final_df_reject)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
